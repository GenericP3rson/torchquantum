<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Build and train a Quantum LSTM. - TorchQuantum 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=362ab14a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">TorchQuantum 0.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">TorchQuantum 0.1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_torchquantum.html">torchquantum</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api_functional.html">torchquantum.functional</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of torchquantum.functional</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.apply_unitary_einsum.html">apply_unitary_einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.apply_unitary_bmm.html">apply_unitary_bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.gate_wrapper.html">gate_wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.reset.html">reset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.hadamard.html">hadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.shadamard.html">shadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.paulix.html">paulix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.pauliy.html">pauliy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.pauliz.html">pauliz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.i.html">i</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.s.html">s</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.t.html">t</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.sx.html">sx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cnot.html">cnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cz.html">cz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cy.html">cy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rx.html">rx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ry.html">ry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rz.html">rz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rxx.html">rxx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ryy.html">ryy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rzz.html">rzz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.zz.html">zz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rzx.html">rzx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.zx.html">zx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.swap.html">swap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.sswap.html">sswap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cswap.html">cswap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.toffoli.html">toffoli</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.phaseshift.html">phaseshift</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.p.html">p</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cp.html">cp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rot.html">rot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.multirz.html">multirz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.crx.html">crx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cry.html">cry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.crz.html">crz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.crot.html">crot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u1.html">u1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u2.html">u2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u3.html">u3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u.html">u</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu1.html">cu1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu2.html">cu2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu3.html">cu3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu.html">cu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.qubitunitary.html">qubitunitary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.qubitunitaryfast.html">qubitunitaryfast</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.qubitunitarystrict.html">qubitunitarystrict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.multicnot.html">multicnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.multixcnot.html">multixcnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.x.html">x</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.y.html">y</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.z.html">z</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.zz.html">zz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cx.html">cx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ccnot.html">ccnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ccx.html">ccx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.reset.html">reset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.singleexcitation.html">singleexcitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ecr.html">ecr</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.echoedcrossresonance.html">echoedcrossresonance</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api_operators.html">torchquantum.operators</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of torchquantum.operators</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.WiresEnum.html">operators.WiresEnum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.NParamsEnum.html">operators.NParamsEnum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.AllWires.html">AllWires</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.AnyWires.html">AnyWires</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.Operator.html">operators.Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.Observable.html">operators.Observable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.Operation.html">operators.Operation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.DiagonalOperation.html">operators.DiagonalOperation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.Hadamard.html">operators.Hadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.SHadamard.html">operators.SHadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.PauliX.html">operators.PauliX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.PauliY.html">operators.PauliY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.PauliZ.html">operators.PauliZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.I.html">operators.I</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.S.html">operators.S</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.T.html">operators.T</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.SX.html">operators.SX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CNOT.html">operators.CNOT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CZ.html">operators.CZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CY.html">operators.CY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.RX.html">operators.RX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.RY.html">operators.RY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.RZ.html">operators.RZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.RXX.html">operators.RXX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.RYY.html">operators.RYY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.RZZ.html">operators.RZZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.RZX.html">operators.RZX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.SWAP.html">operators.SWAP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.SSWAP.html">operators.SSWAP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CSWAP.html">operators.CSWAP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.Toffoli.html">operators.Toffoli</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.PhaseShift.html">operators.PhaseShift</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.Rot.html">operators.Rot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.MultiRZ.html">operators.MultiRZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CRX.html">operators.CRX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CRY.html">operators.CRY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CRZ.html">operators.CRZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CRot.html">operators.CRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.U1.html">operators.U1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.U2.html">operators.U2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.U3.html">operators.U3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CU1.html">operators.CU1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CU2.html">operators.CU2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.CU3.html">operators.CU3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.QubitUnitary.html">operators.QubitUnitary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.QubitUnitaryFast.html">operators.QubitUnitaryFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.TrainableUnitary.html">operators.TrainableUnitary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.TrainableUnitaryStrict.html">operators.TrainableUnitaryStrict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.MultiCNOT.html">operators.MultiCNOT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.MultiXCNOT.html">operators.MultiXCNOT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.Reset.html">operators.Reset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.SingleExcitation.html">operators.SingleExcitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.EchoedCrossResonance.html">operators.EchoedCrossResonance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operators.ECR.html">operators.ECR</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api_layers.html">torchquantum.layers</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of torchquantum.layers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.QuantumModuleFromOps.html">layers.QuantumModuleFromOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.TrainableOpAll.html">layers.TrainableOpAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.ClassicalInOpAll.html">layers.ClassicalInOpAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.FixedOpAll.html">layers.FixedOpAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.TwoQAll.html">layers.TwoQAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.RandomLayer.html">layers.RandomLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.RandomLayerAllTypes.html">layers.RandomLayerAllTypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.Op1QAllLayer.html">layers.Op1QAllLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.RandomOp1All.html">layers.RandomOp1All</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.Op2QAllLayer.html">layers.Op2QAllLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.Op2QButterflyLayer.html">layers.Op2QButterflyLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.Op2QDenseLayer.html">layers.Op2QDenseLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.CXLayer.html">layers.CXLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.CXCXCXLayer.html">layers.CXCXCXLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.SWAPSWAPLayer.html">layers.SWAPSWAPLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.RXYZCXLayer0.html">layers.RXYZCXLayer0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layers.QFTLayer.html">layers.QFTLayer</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../usage_installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../index.html">TorchQuantum Examples</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of TorchQuantum Examples</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../gradient_pruning/probabilistic_gradient_pruning.html">Probabilistic gradient pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../param_shift_onchip_training/param_shift_onchip_training.html">Apply parameters shift rules to train quantum model using TorchQuantum.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quantum_kernel_method/quantum_kernel_method.html">Quantum Kernel Methods for IRIS dataset classification with TorchQuantum.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quanvolution/quanvolution.html">Quanvolution (Quantum convolution) for MNIST image classification with TorchQuantum.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../superdense_coding/superdense_coding_torchquantum.html">Superdense Coding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../superdense_coding/superdense_coding_torchquantum.html#References:">References:</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <p>Tutorial Author:Mohammadreza Tavasoli Naeini Advisor:Hanrui Wang</p>
<p>This tutorial shows; how to use the TrochQuantum library in building a quantum transformer. This notebook is adapted, excerpted from <a class="reference external" href="https://github.com/rdisipio/qlstm">https://github.com/rdisipio/qlstm</a>, written by Riccardo Di Sipio. In this notebook, we use the TorchQuantum library instead of the Pennylane library to build a quantum machine learning model.</p>
<section id="Build-and-train-a-Quantum-LSTM.">
<h1>Build and train a Quantum LSTM.<a class="headerlink" href="#Build-and-train-a-Quantum-LSTM." title="Link to this heading">#</a></h1>
</section>
<section id="Installation">
<h1>Installation<a class="headerlink" href="#Installation" title="Link to this heading">#</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mit-han-lab/torchquantum.git
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
fatal: destination path &#39;torchquantum&#39; already exists and is not an empty directory.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">cd</span> torchquantum
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
/Users/mohammad/qlstm/torchquantum
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--editable<span class="w"> </span>.
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Obtaining file:///Users/mohammad/qlstm/torchquantum
Requirement already satisfied: numpy&gt;=1.19.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (1.20.1)
Requirement already satisfied: torchvision&gt;=0.9.0.dev20210130 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.9.1)
Requirement already satisfied: tqdm&gt;=4.56.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (4.60.0)
Requirement already satisfied: setuptools&gt;=52.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (65.3.0)
Requirement already satisfied: torch&gt;=1.8.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (1.8.1)
Requirement already satisfied: torchpack&gt;=0.3.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.3.1)
Requirement already satisfied: qiskit&gt;=0.32.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.38.0)
Requirement already satisfied: matplotlib&gt;=3.3.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (3.4.1)
Requirement already satisfied: pathos&gt;=0.2.7 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.2.9)
Requirement already satisfied: pylatexenc&gt;=2.10 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (2.10)
Requirement already satisfied: cycler&gt;=0.10 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (0.10.0)
Requirement already satisfied: pillow&gt;=6.2.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (8.2.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (1.3.1)
Requirement already satisfied: pyparsing&gt;=2.2.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (2.4.7)
Requirement already satisfied: python-dateutil&gt;=2.7 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (2.8.1)
Requirement already satisfied: six in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (1.15.0)
Requirement already satisfied: dill&gt;=0.3.5.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (0.3.5.1)
Requirement already satisfied: multiprocess&gt;=0.70.13 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (0.70.13)
Requirement already satisfied: ppft&gt;=1.7.6.5 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (1.7.6.5)
Requirement already satisfied: pox&gt;=0.3.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (0.3.1)
Requirement already satisfied: qiskit-aer==0.11.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.11.0)
Requirement already satisfied: qiskit-ibmq-provider==0.19.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.19.2)
Requirement already satisfied: qiskit-terra==0.21.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.21.2)
Requirement already satisfied: scipy&gt;=1.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-aer==0.11.0-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.6.1)
Requirement already satisfied: requests-ntlm&gt;=1.1.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.1.0)
Requirement already satisfied: urllib3&gt;=1.21.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.26.4)
Requirement already satisfied: websockets&gt;=10.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (10.3)
Requirement already satisfied: websocket-client&gt;=1.0.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.4.1)
Requirement already satisfied: requests&gt;=2.19 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2.25.1)
Requirement already satisfied: ply&gt;=3.10 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (3.11)
Requirement already satisfied: retworkx&gt;=0.11.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.11.0)
Requirement already satisfied: stevedore&gt;=3.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (4.0.0)
Requirement already satisfied: psutil&gt;=5 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (5.8.0)
Requirement already satisfied: tweedledum&lt;2.0,&gt;=1.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.1.1)
Requirement already satisfied: sympy&gt;=1.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.7.1)
Requirement already satisfied: symengine&gt;=0.9 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.9.2)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests&gt;=2.19-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (4.0.0)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests&gt;=2.19-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests&gt;=2.19-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2020.12.5)
Requirement already satisfied: ntlm-auth&gt;=1.0.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.5.0)
Requirement already satisfied: cryptography&gt;=1.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (3.4.7)
Requirement already satisfied: cffi&gt;=1.12 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from cryptography&gt;=1.3-&gt;requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.14.5)
Requirement already satisfied: pycparser in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=1.3-&gt;requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2.20)
Requirement already satisfied: pbr!=2.1.0,&gt;=2.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from stevedore&gt;=3.0.0-&gt;qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (5.10.0)
Requirement already satisfied: mpmath&gt;=0.19 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from sympy&gt;=1.3-&gt;qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.2.1)
Requirement already satisfied: typing-extensions in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torch&gt;=1.8.0-&gt;torchquantum==0.1.2) (3.7.4.3)
Requirement already satisfied: loguru in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.6.0)
Requirement already satisfied: multimethod in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.8)
Requirement already satisfied: pyyaml in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (5.3.1)
Requirement already satisfied: tensorboard in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (2.9.1)
Requirement already satisfied: tensorpack in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.11)
Requirement already satisfied: h5py in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.1.0)
Requirement already satisfied: toml in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.10.2)
Requirement already satisfied: protobuf&lt;3.20,&gt;=3.9.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.15.7)
Requirement already satisfied: grpcio&gt;=1.24.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.37.0)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.28.0)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.6.1)
Requirement already satisfied: wheel&gt;=0.26 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.36.2)
Requirement already satisfied: werkzeug&gt;=1.0.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.0.1)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.4.4)
Requirement already satisfied: markdown&gt;=2.6.8 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.3.4)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.8.0)
Requirement already satisfied: absl-py&gt;=0.4 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.2.0)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.2.8)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (4.7.2)
Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (4.2.1)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.3.0)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.4.8)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.1.0)
Requirement already satisfied: msgpack&gt;=0.5.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.0.4)
Requirement already satisfied: msgpack-numpy&gt;=0.4.4.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.4.8)
Requirement already satisfied: tabulate&gt;=0.7.7 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.8.10)
Requirement already satisfied: pyzmq&gt;=16 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (22.3.0)
Requirement already satisfied: termcolor&gt;=1.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.1.0)
Installing collected packages: torchquantum
  Attempting uninstall: torchquantum
    Found existing installation: torchquantum 0.1.2
    Uninstalling torchquantum-0.1.2:
      Successfully uninstalled torchquantum-0.1.2
  Running setup.py develop for torchquantum
Successfully installed torchquantum
</pre></div></div>
</div>
</section>
<section id="Setup">
<h1>Setup<a class="headerlink" href="#Setup" title="Link to this heading">#</a></h1>
<p>Our code requires torchquantum lib,and pytorch. We need torch function from torch.nn.functional, optimizers(optim), torchquantum module.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchquantum</span> <span class="k">as</span> <span class="nn">tq</span>
<span class="kn">import</span> <span class="nn">torchquantum.functional</span> <span class="k">as</span> <span class="nn">tqf</span>
</pre></div>
</div>
</div>
</section>
<section id="Build-Quantum-LSTM">
<h1>Build Quantum LSTM<a class="headerlink" href="#Build-Quantum-LSTM" title="Link to this heading">#</a></h1>
<p>Before the rise of the transformer, recurrent neural networks, especially Long Short-Term Memory (LSTM), were the most successful techniques for generating and analyzing sequential data. LSTM uses a combination of “memory” and “statefulness” tricks to understand which parts of the input are relevant to compute the output.[1]</p>
<p><img alt="Screen%20Shot%202022-09-15%20at%2010.25.20%20PM-2.png" src="data:image/png;base64,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" /></p>
<figcaption align="center"><p>LSTM architecture</p>
</figcaption><p>To convert classical LSTM to quantum-enhanced LSTM (QLSTM) each of the classical linear operations Wf, Wi, WC, and Wo is replaced by a hybrid quantum-classical component that consists of a Variational Quantum Circuit sandwiched between classical layers. [1]</p>
<p>Typically a Quantum Neural Network module consists of three parts: encoder, ansatz, and measurement. We can create an encoder by passing a list of gates to tq.GeneralEncoder. Each entry in the list contains input_idx, func, and wires. Here, each qubit has a rotation-X gate. 4 RX gates in total. They can encode the 4 input data to the quantum state. Then we choose ansatz such that they are entangled between each other, rotated by an arbitrary angle. Finally, we perform Pauli-Z measurements on
each qubit by creating a tq.MeasureAll module and passing tq.PauliZ to it. The measure function will return four expectation values from four qubits.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># use &#39;qiskit.ibmq&#39; instead to run on hardware</span>
    <span class="k">class</span> <span class="nc">QLayer_forget</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">QuantumModule</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">GeneralEncoder</span><span class="p">(</span>
        <span class="p">[</span>   <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]},</span>
        <span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">measure</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">MeasureAll</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">qdev</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">(</span><span class="n">n_wires</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">,</span> <span class="n">bsz</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="n">qdev</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">QLayer_input</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">QuantumModule</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">GeneralEncoder</span><span class="p">(</span>
        <span class="p">[</span>   <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]},</span>
        <span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">measure</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">MeasureAll</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">qdev</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">(</span><span class="n">n_wires</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">,</span> <span class="n">bsz</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="n">qdev</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">QLayer_update</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">QuantumModule</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">GeneralEncoder</span><span class="p">(</span>
        <span class="p">[</span>   <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]},</span>
        <span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">measure</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">MeasureAll</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">qdev</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">(</span><span class="n">n_wires</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">,</span> <span class="n">bsz</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="n">qdev</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">QLayer_output</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">QuantumModule</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">GeneralEncoder</span><span class="p">(</span>
        <span class="p">[</span>   <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]},</span>
        <span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">measure</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">MeasureAll</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">qdev</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">(</span><span class="n">n_wires</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">,</span> <span class="n">bsz</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="n">qdev</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="n">qdev</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">input_size</span><span class="p">,</span>
                <span class="n">hidden_size</span><span class="p">,</span>
                <span class="n">n_qubits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                <span class="n">n_qlayers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">return_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;default.qubit&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">QLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qubits</span> <span class="o">=</span> <span class="n">n_qubits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qlayers</span> <span class="o">=</span> <span class="n">n_qlayers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>  <span class="c1"># &quot;default.qubit&quot;, &quot;qiskit.basicaer&quot;, &quot;qiskit.ibm&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_sequences</span> <span class="o">=</span> <span class="n">return_sequences</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_state</span> <span class="o">=</span> <span class="n">return_state</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">clayer_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">concat_size</span><span class="p">,</span> <span class="n">n_qubits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">VQC</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;forget&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">QLayer_forget</span><span class="p">(),</span>
            <span class="s1">&#39;input&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">QLayer_input</span><span class="p">(),</span>
            <span class="s1">&#39;update&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">QLayer_update</span><span class="p">(),</span>
            <span class="s1">&#39;output&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">QLayer_output</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clayer_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_qubits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="c1">#self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">init_states</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        x.shape is (batch_size, seq_length, feature_size)</span>
<span class="sd">        recurrent_activation -&gt; sigmoid</span>
<span class="sd">        activation -&gt; tanh</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">features_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">features_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="n">hidden_seq</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">init_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># hidden state (output)</span>
            <span class="n">c_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># cell state</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># for now we ignore the fact that in PyTorch you can stack multiple RNNs</span>
            <span class="c1"># so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]</span>
            <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="n">init_states</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">h_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">c_t</span> <span class="o">=</span> <span class="n">c_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_length</span><span class="p">):</span>
            <span class="c1"># get features from the t-th element in seq, for all entries in the batch</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># Concatenate input and hidden state</span>
            <span class="n">v_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">h_t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># match qubit dimension</span>
            <span class="n">y_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clayer_in</span><span class="p">(</span><span class="n">v_t</span><span class="p">)</span>

            <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clayer_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">VQC</span><span class="p">[</span><span class="s1">&#39;forget&#39;</span><span class="p">](</span><span class="n">y_t</span><span class="p">)))</span>  <span class="c1"># forget block</span>
            <span class="n">i_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clayer_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">VQC</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">](</span><span class="n">y_t</span><span class="p">)))</span>  <span class="c1"># input block</span>
            <span class="n">g_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clayer_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">VQC</span><span class="p">[</span><span class="s1">&#39;update&#39;</span><span class="p">](</span><span class="n">y_t</span><span class="p">)))</span>  <span class="c1"># update block</span>
            <span class="n">o_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clayer_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">VQC</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">](</span><span class="n">y_t</span><span class="p">)))</span> <span class="c1"># output block</span>

            <span class="n">c_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">f_t</span> <span class="o">*</span> <span class="n">c_t</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">i_t</span> <span class="o">*</span> <span class="n">g_t</span><span class="p">)</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>

            <span class="n">hidden_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h_t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">hidden_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hidden_seq</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">hidden_seq</span> <span class="o">=</span> <span class="n">hidden_seq</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">hidden_seq</span><span class="p">,</span> <span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
<br/><br/></pre></div>
</div>
</div>
</section>
<section id="POS_tagging">
<h1>POS_tagging<a class="headerlink" href="#POS_tagging" title="Link to this heading">#</a></h1>
<p>Here we define the possible tags: determinant, noun, verb.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tag_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;DET&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;NN&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>  <span class="c1"># Assign each tag with a unique index</span>
<span class="n">ix_to_tag</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="n">tag_to_ix</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>
</div>
</div>
<p>The function below tokenizes the sentence and matches the label to each word.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_sequence</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">to_ix</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_ix</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now we can prepare the input dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Tags are: DET - determiner; NN - noun; V - verb</span>
    <span class="c1"># For example, the word &quot;The&quot; is a determiner</span>
    <span class="p">(</span><span class="s2">&quot;The dog ate the apple&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="p">[</span><span class="s2">&quot;DET&quot;</span><span class="p">,</span> <span class="s2">&quot;NN&quot;</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="s2">&quot;DET&quot;</span><span class="p">,</span> <span class="s2">&quot;NN&quot;</span><span class="p">]),</span>
    <span class="p">(</span><span class="s2">&quot;Everybody read that book&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="p">[</span><span class="s2">&quot;NN&quot;</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="s2">&quot;DET&quot;</span><span class="p">,</span> <span class="s2">&quot;NN&quot;</span><span class="p">])</span>
<span class="p">]</span>
<span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># For each words-list (sentence) and tags-list in each tuple of training_data</span>
<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_to_ix</span><span class="p">:</span>  <span class="c1"># word has not been assigned an index yet</span>
            <span class="n">word_to_ix</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>  <span class="c1"># Assign each word with a unique index</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary: </span><span class="si">{</span><span class="n">word_to_ix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Entities: </span><span class="si">{</span><span class="n">ix_to_tag</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Vocabulary: {&#39;The&#39;: 0, &#39;dog&#39;: 1, &#39;ate&#39;: 2, &#39;the&#39;: 3, &#39;apple&#39;: 4, &#39;Everybody&#39;: 5, &#39;read&#39;: 6, &#39;that&#39;: 7, &#39;book&#39;: 8}
Entities: {0: &#39;DET&#39;, 1: &#39;NN&#39;, 2: &#39;V&#39;}
</pre></div></div>
</div>
<p>The idea is to pass the two sequences through the LSTM, which will output the hidden array of vectors [h_0, h_1, h_2, h_3, h_4], one for each word. A dense layer “head” is attached to the LSTM’s outputs to calculate the probability that each word may be a determinant, noun or verb.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LSTMTagger</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">tagset_size</span><span class="p">,</span> <span class="n">n_qubits</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTMTagger</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

        <span class="c1"># The LSTM takes word embeddings as inputs, and outputs hidden states</span>
        <span class="c1"># with dimensionality hidden_dim.</span>
        <span class="k">if</span> <span class="n">n_qubits</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tagger will use Quantum LSTM&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">QLSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_qubits</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tagger will use Classical LSTM&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

        <span class="c1"># The linear layer that maps from hidden state space to tag space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden2tag</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">tagset_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">tag_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden2tag</span><span class="p">(</span><span class="n">lstm_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">tag_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">tag_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tag_scores</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_classical</span> <span class="o">=</span> <span class="n">LSTMTagger</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span>
                        <span class="n">hidden_dim</span><span class="p">,</span>
                        <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">),</span>
                        <span class="n">tagset_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tag_to_ix</span><span class="p">),</span>
                        <span class="n">n_qubits</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tagger will use Classical LSTM
</pre></div></div>
</div>
</section>
<section id="Training">
<h1>Training<a class="headerlink" href="#Training" title="Link to this heading">#</a></h1>
<p>Following the example from the PyTorch website, we train the two networks (classical and quantum LSTM) for 300 epochs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">,</span> <span class="n">tag_to_ix</span><span class="p">):</span>
    <span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="p">[]</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
            <span class="c1"># Step 1. Remember that Pytorch accumulates gradients.</span>
            <span class="c1"># We need to clear them out before each instance</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Step 2. Get our inputs ready for the network, that is, turn them into</span>
            <span class="c1"># Tensors of word indices.</span>
            <span class="n">sentence_in</span> <span class="o">=</span> <span class="n">prepare_sequence</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">prepare_sequence</span><span class="p">(</span><span class="n">tags</span><span class="p">,</span> <span class="n">tag_to_ix</span><span class="p">)</span>

            <span class="c1"># Step 3. Run our forward pass.</span>
            <span class="n">tag_scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">sentence_in</span><span class="p">)</span>

            <span class="c1"># Step 4. Compute the loss, gradients, and update the parameters by</span>
            <span class="c1">#  calling optimizer.step()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">tag_scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>

            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tag_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>

        <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
        <span class="n">corrects</span> <span class="o">=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">corrects</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="s2">: Loss = </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> Acc = </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">history</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history_classical</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model_classical</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">,</span> <span class="n">tag_to_ix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch 1 / 300: Loss = 1.116 Acc = 0.22
Epoch 2 / 300: Loss = 1.102 Acc = 0.22
Epoch 3 / 300: Loss = 1.089 Acc = 0.56
Epoch 4 / 300: Loss = 1.078 Acc = 0.67
Epoch 5 / 300: Loss = 1.068 Acc = 0.56
Epoch 6 / 300: Loss = 1.058 Acc = 0.67
Epoch 7 / 300: Loss = 1.049 Acc = 0.56
Epoch 8 / 300: Loss = 1.041 Acc = 0.67
Epoch 9 / 300: Loss = 1.034 Acc = 0.67
Epoch 10 / 300: Loss = 1.027 Acc = 0.67
Epoch 11 / 300: Loss = 1.020 Acc = 0.67
Epoch 12 / 300: Loss = 1.013 Acc = 0.67
Epoch 13 / 300: Loss = 1.007 Acc = 0.56
Epoch 14 / 300: Loss = 1.001 Acc = 0.56
Epoch 15 / 300: Loss = 0.995 Acc = 0.44
Epoch 16 / 300: Loss = 0.989 Acc = 0.44
Epoch 17 / 300: Loss = 0.984 Acc = 0.44
Epoch 18 / 300: Loss = 0.978 Acc = 0.56
Epoch 19 / 300: Loss = 0.973 Acc = 0.56
Epoch 20 / 300: Loss = 0.967 Acc = 0.56
Epoch 21 / 300: Loss = 0.961 Acc = 0.56
Epoch 22 / 300: Loss = 0.956 Acc = 0.56
Epoch 23 / 300: Loss = 0.950 Acc = 0.56
Epoch 24 / 300: Loss = 0.944 Acc = 0.67
Epoch 25 / 300: Loss = 0.939 Acc = 0.67
Epoch 26 / 300: Loss = 0.933 Acc = 0.67
Epoch 27 / 300: Loss = 0.927 Acc = 0.78
Epoch 28 / 300: Loss = 0.921 Acc = 0.78
Epoch 29 / 300: Loss = 0.914 Acc = 0.78
Epoch 30 / 300: Loss = 0.908 Acc = 0.78
Epoch 31 / 300: Loss = 0.902 Acc = 0.78
Epoch 32 / 300: Loss = 0.895 Acc = 0.78
Epoch 33 / 300: Loss = 0.888 Acc = 0.78
Epoch 34 / 300: Loss = 0.881 Acc = 0.78
Epoch 35 / 300: Loss = 0.874 Acc = 0.78
Epoch 36 / 300: Loss = 0.867 Acc = 0.78
Epoch 37 / 300: Loss = 0.860 Acc = 0.78
Epoch 38 / 300: Loss = 0.853 Acc = 0.78
Epoch 39 / 300: Loss = 0.845 Acc = 0.78
Epoch 40 / 300: Loss = 0.837 Acc = 0.78
Epoch 41 / 300: Loss = 0.829 Acc = 0.78
Epoch 42 / 300: Loss = 0.821 Acc = 0.78
Epoch 43 / 300: Loss = 0.813 Acc = 0.78
Epoch 44 / 300: Loss = 0.805 Acc = 0.78
Epoch 45 / 300: Loss = 0.796 Acc = 0.78
Epoch 46 / 300: Loss = 0.788 Acc = 0.78
Epoch 47 / 300: Loss = 0.779 Acc = 0.78
Epoch 48 / 300: Loss = 0.770 Acc = 0.78
Epoch 49 / 300: Loss = 0.762 Acc = 0.78
Epoch 50 / 300: Loss = 0.753 Acc = 0.78
Epoch 51 / 300: Loss = 0.744 Acc = 0.78
Epoch 52 / 300: Loss = 0.735 Acc = 0.78
Epoch 53 / 300: Loss = 0.725 Acc = 0.78
Epoch 54 / 300: Loss = 0.716 Acc = 0.78
Epoch 55 / 300: Loss = 0.707 Acc = 0.78
Epoch 56 / 300: Loss = 0.698 Acc = 0.78
Epoch 57 / 300: Loss = 0.689 Acc = 0.78
Epoch 58 / 300: Loss = 0.680 Acc = 0.78
Epoch 59 / 300: Loss = 0.670 Acc = 0.78
Epoch 60 / 300: Loss = 0.661 Acc = 0.78
Epoch 61 / 300: Loss = 0.652 Acc = 0.78
Epoch 62 / 300: Loss = 0.643 Acc = 0.78
Epoch 63 / 300: Loss = 0.634 Acc = 0.78
Epoch 64 / 300: Loss = 0.625 Acc = 0.78
Epoch 65 / 300: Loss = 0.616 Acc = 0.78
Epoch 66 / 300: Loss = 0.607 Acc = 0.78
Epoch 67 / 300: Loss = 0.599 Acc = 0.78
Epoch 68 / 300: Loss = 0.590 Acc = 0.78
Epoch 69 / 300: Loss = 0.581 Acc = 0.78
Epoch 70 / 300: Loss = 0.573 Acc = 0.78
Epoch 71 / 300: Loss = 0.564 Acc = 0.78
Epoch 72 / 300: Loss = 0.556 Acc = 0.78
Epoch 73 / 300: Loss = 0.548 Acc = 0.78
Epoch 74 / 300: Loss = 0.539 Acc = 0.78
Epoch 75 / 300: Loss = 0.531 Acc = 0.78
Epoch 76 / 300: Loss = 0.523 Acc = 0.78
Epoch 77 / 300: Loss = 0.515 Acc = 0.78
Epoch 78 / 300: Loss = 0.507 Acc = 0.78
Epoch 79 / 300: Loss = 0.499 Acc = 0.78
Epoch 80 / 300: Loss = 0.491 Acc = 0.89
Epoch 81 / 300: Loss = 0.483 Acc = 0.89
Epoch 82 / 300: Loss = 0.476 Acc = 0.89
Epoch 83 / 300: Loss = 0.468 Acc = 1.00
Epoch 84 / 300: Loss = 0.460 Acc = 1.00
Epoch 85 / 300: Loss = 0.453 Acc = 1.00
Epoch 86 / 300: Loss = 0.445 Acc = 1.00
Epoch 87 / 300: Loss = 0.438 Acc = 1.00
Epoch 88 / 300: Loss = 0.430 Acc = 1.00
Epoch 89 / 300: Loss = 0.423 Acc = 1.00
Epoch 90 / 300: Loss = 0.416 Acc = 1.00
Epoch 91 / 300: Loss = 0.408 Acc = 1.00
Epoch 92 / 300: Loss = 0.401 Acc = 1.00
Epoch 93 / 300: Loss = 0.394 Acc = 1.00
Epoch 94 / 300: Loss = 0.387 Acc = 1.00
Epoch 95 / 300: Loss = 0.380 Acc = 1.00
Epoch 96 / 300: Loss = 0.373 Acc = 1.00
Epoch 97 / 300: Loss = 0.366 Acc = 1.00
Epoch 98 / 300: Loss = 0.360 Acc = 1.00
Epoch 99 / 300: Loss = 0.353 Acc = 1.00
Epoch 100 / 300: Loss = 0.346 Acc = 1.00
Epoch 101 / 300: Loss = 0.340 Acc = 1.00
Epoch 102 / 300: Loss = 0.333 Acc = 1.00
Epoch 103 / 300: Loss = 0.327 Acc = 1.00
Epoch 104 / 300: Loss = 0.321 Acc = 1.00
Epoch 105 / 300: Loss = 0.314 Acc = 1.00
Epoch 106 / 300: Loss = 0.308 Acc = 1.00
Epoch 107 / 300: Loss = 0.302 Acc = 1.00
Epoch 108 / 300: Loss = 0.296 Acc = 1.00
Epoch 109 / 300: Loss = 0.290 Acc = 1.00
Epoch 110 / 300: Loss = 0.285 Acc = 1.00
Epoch 111 / 300: Loss = 0.279 Acc = 1.00
Epoch 112 / 300: Loss = 0.273 Acc = 1.00
Epoch 113 / 300: Loss = 0.268 Acc = 1.00
Epoch 114 / 300: Loss = 0.262 Acc = 1.00
Epoch 115 / 300: Loss = 0.257 Acc = 1.00
Epoch 116 / 300: Loss = 0.252 Acc = 1.00
Epoch 117 / 300: Loss = 0.247 Acc = 1.00
Epoch 118 / 300: Loss = 0.242 Acc = 1.00
Epoch 119 / 300: Loss = 0.237 Acc = 1.00
Epoch 120 / 300: Loss = 0.232 Acc = 1.00
Epoch 121 / 300: Loss = 0.227 Acc = 1.00
Epoch 122 / 300: Loss = 0.223 Acc = 1.00
Epoch 123 / 300: Loss = 0.218 Acc = 1.00
Epoch 124 / 300: Loss = 0.214 Acc = 1.00
Epoch 125 / 300: Loss = 0.209 Acc = 1.00
Epoch 126 / 300: Loss = 0.205 Acc = 1.00
Epoch 127 / 300: Loss = 0.201 Acc = 1.00
Epoch 128 / 300: Loss = 0.197 Acc = 1.00
Epoch 129 / 300: Loss = 0.193 Acc = 1.00
Epoch 130 / 300: Loss = 0.189 Acc = 1.00
Epoch 131 / 300: Loss = 0.185 Acc = 1.00
Epoch 132 / 300: Loss = 0.182 Acc = 1.00
Epoch 133 / 300: Loss = 0.178 Acc = 1.00
Epoch 134 / 300: Loss = 0.174 Acc = 1.00
Epoch 135 / 300: Loss = 0.171 Acc = 1.00
Epoch 136 / 300: Loss = 0.168 Acc = 1.00
Epoch 137 / 300: Loss = 0.164 Acc = 1.00
Epoch 138 / 300: Loss = 0.161 Acc = 1.00
Epoch 139 / 300: Loss = 0.158 Acc = 1.00
Epoch 140 / 300: Loss = 0.155 Acc = 1.00
Epoch 141 / 300: Loss = 0.152 Acc = 1.00
Epoch 142 / 300: Loss = 0.149 Acc = 1.00
Epoch 143 / 300: Loss = 0.146 Acc = 1.00
Epoch 144 / 300: Loss = 0.143 Acc = 1.00
Epoch 145 / 300: Loss = 0.141 Acc = 1.00
Epoch 146 / 300: Loss = 0.138 Acc = 1.00
Epoch 147 / 300: Loss = 0.135 Acc = 1.00
Epoch 148 / 300: Loss = 0.133 Acc = 1.00
Epoch 149 / 300: Loss = 0.130 Acc = 1.00
Epoch 150 / 300: Loss = 0.128 Acc = 1.00
Epoch 151 / 300: Loss = 0.126 Acc = 1.00
Epoch 152 / 300: Loss = 0.123 Acc = 1.00
Epoch 153 / 300: Loss = 0.121 Acc = 1.00
Epoch 154 / 300: Loss = 0.119 Acc = 1.00
Epoch 155 / 300: Loss = 0.117 Acc = 1.00
Epoch 156 / 300: Loss = 0.115 Acc = 1.00
Epoch 157 / 300: Loss = 0.113 Acc = 1.00
Epoch 158 / 300: Loss = 0.111 Acc = 1.00
Epoch 159 / 300: Loss = 0.109 Acc = 1.00
Epoch 160 / 300: Loss = 0.107 Acc = 1.00
Epoch 161 / 300: Loss = 0.105 Acc = 1.00
Epoch 162 / 300: Loss = 0.104 Acc = 1.00
Epoch 163 / 300: Loss = 0.102 Acc = 1.00
Epoch 164 / 300: Loss = 0.100 Acc = 1.00
Epoch 165 / 300: Loss = 0.099 Acc = 1.00
Epoch 166 / 300: Loss = 0.097 Acc = 1.00
Epoch 167 / 300: Loss = 0.096 Acc = 1.00
Epoch 168 / 300: Loss = 0.094 Acc = 1.00
Epoch 169 / 300: Loss = 0.093 Acc = 1.00
Epoch 170 / 300: Loss = 0.091 Acc = 1.00
Epoch 171 / 300: Loss = 0.090 Acc = 1.00
Epoch 172 / 300: Loss = 0.088 Acc = 1.00
Epoch 173 / 300: Loss = 0.087 Acc = 1.00
Epoch 174 / 300: Loss = 0.086 Acc = 1.00
Epoch 175 / 300: Loss = 0.084 Acc = 1.00
Epoch 176 / 300: Loss = 0.083 Acc = 1.00
Epoch 177 / 300: Loss = 0.082 Acc = 1.00
Epoch 178 / 300: Loss = 0.081 Acc = 1.00
Epoch 179 / 300: Loss = 0.079 Acc = 1.00
Epoch 180 / 300: Loss = 0.078 Acc = 1.00
Epoch 181 / 300: Loss = 0.077 Acc = 1.00
Epoch 182 / 300: Loss = 0.076 Acc = 1.00
Epoch 183 / 300: Loss = 0.075 Acc = 1.00
Epoch 184 / 300: Loss = 0.074 Acc = 1.00
Epoch 185 / 300: Loss = 0.073 Acc = 1.00
Epoch 186 / 300: Loss = 0.072 Acc = 1.00
Epoch 187 / 300: Loss = 0.071 Acc = 1.00
Epoch 188 / 300: Loss = 0.070 Acc = 1.00
Epoch 189 / 300: Loss = 0.069 Acc = 1.00
Epoch 190 / 300: Loss = 0.068 Acc = 1.00
Epoch 191 / 300: Loss = 0.067 Acc = 1.00
Epoch 192 / 300: Loss = 0.066 Acc = 1.00
Epoch 193 / 300: Loss = 0.066 Acc = 1.00
Epoch 194 / 300: Loss = 0.065 Acc = 1.00
Epoch 195 / 300: Loss = 0.064 Acc = 1.00
Epoch 196 / 300: Loss = 0.063 Acc = 1.00
Epoch 197 / 300: Loss = 0.062 Acc = 1.00
Epoch 198 / 300: Loss = 0.062 Acc = 1.00
Epoch 199 / 300: Loss = 0.061 Acc = 1.00
Epoch 200 / 300: Loss = 0.060 Acc = 1.00
Epoch 201 / 300: Loss = 0.059 Acc = 1.00
Epoch 202 / 300: Loss = 0.059 Acc = 1.00
Epoch 203 / 300: Loss = 0.058 Acc = 1.00
Epoch 204 / 300: Loss = 0.057 Acc = 1.00
Epoch 205 / 300: Loss = 0.057 Acc = 1.00
Epoch 206 / 300: Loss = 0.056 Acc = 1.00
Epoch 207 / 300: Loss = 0.055 Acc = 1.00
Epoch 208 / 300: Loss = 0.055 Acc = 1.00
Epoch 209 / 300: Loss = 0.054 Acc = 1.00
Epoch 210 / 300: Loss = 0.053 Acc = 1.00
Epoch 211 / 300: Loss = 0.053 Acc = 1.00
Epoch 212 / 300: Loss = 0.052 Acc = 1.00
Epoch 213 / 300: Loss = 0.052 Acc = 1.00
Epoch 214 / 300: Loss = 0.051 Acc = 1.00
Epoch 215 / 300: Loss = 0.051 Acc = 1.00
Epoch 216 / 300: Loss = 0.050 Acc = 1.00
Epoch 217 / 300: Loss = 0.049 Acc = 1.00
Epoch 218 / 300: Loss = 0.049 Acc = 1.00
Epoch 219 / 300: Loss = 0.048 Acc = 1.00
Epoch 220 / 300: Loss = 0.048 Acc = 1.00
Epoch 221 / 300: Loss = 0.047 Acc = 1.00
Epoch 222 / 300: Loss = 0.047 Acc = 1.00
Epoch 223 / 300: Loss = 0.046 Acc = 1.00
Epoch 224 / 300: Loss = 0.046 Acc = 1.00
Epoch 225 / 300: Loss = 0.046 Acc = 1.00
Epoch 226 / 300: Loss = 0.045 Acc = 1.00
Epoch 227 / 300: Loss = 0.045 Acc = 1.00
Epoch 228 / 300: Loss = 0.044 Acc = 1.00
Epoch 229 / 300: Loss = 0.044 Acc = 1.00
Epoch 230 / 300: Loss = 0.043 Acc = 1.00
Epoch 231 / 300: Loss = 0.043 Acc = 1.00
Epoch 232 / 300: Loss = 0.043 Acc = 1.00
Epoch 233 / 300: Loss = 0.042 Acc = 1.00
Epoch 234 / 300: Loss = 0.042 Acc = 1.00
Epoch 235 / 300: Loss = 0.041 Acc = 1.00
Epoch 236 / 300: Loss = 0.041 Acc = 1.00
Epoch 237 / 300: Loss = 0.041 Acc = 1.00
Epoch 238 / 300: Loss = 0.040 Acc = 1.00
Epoch 239 / 300: Loss = 0.040 Acc = 1.00
Epoch 240 / 300: Loss = 0.039 Acc = 1.00
Epoch 241 / 300: Loss = 0.039 Acc = 1.00
Epoch 242 / 300: Loss = 0.039 Acc = 1.00
Epoch 243 / 300: Loss = 0.038 Acc = 1.00
Epoch 244 / 300: Loss = 0.038 Acc = 1.00
Epoch 245 / 300: Loss = 0.038 Acc = 1.00
Epoch 246 / 300: Loss = 0.037 Acc = 1.00
Epoch 247 / 300: Loss = 0.037 Acc = 1.00
Epoch 248 / 300: Loss = 0.037 Acc = 1.00
Epoch 249 / 300: Loss = 0.036 Acc = 1.00
Epoch 250 / 300: Loss = 0.036 Acc = 1.00
Epoch 251 / 300: Loss = 0.036 Acc = 1.00
Epoch 252 / 300: Loss = 0.036 Acc = 1.00
Epoch 253 / 300: Loss = 0.035 Acc = 1.00
Epoch 254 / 300: Loss = 0.035 Acc = 1.00
Epoch 255 / 300: Loss = 0.035 Acc = 1.00
Epoch 256 / 300: Loss = 0.034 Acc = 1.00
Epoch 257 / 300: Loss = 0.034 Acc = 1.00
Epoch 258 / 300: Loss = 0.034 Acc = 1.00
Epoch 259 / 300: Loss = 0.034 Acc = 1.00
Epoch 260 / 300: Loss = 0.033 Acc = 1.00
Epoch 261 / 300: Loss = 0.033 Acc = 1.00
Epoch 262 / 300: Loss = 0.033 Acc = 1.00
Epoch 263 / 300: Loss = 0.033 Acc = 1.00
Epoch 264 / 300: Loss = 0.032 Acc = 1.00
Epoch 265 / 300: Loss = 0.032 Acc = 1.00
Epoch 266 / 300: Loss = 0.032 Acc = 1.00
Epoch 267 / 300: Loss = 0.032 Acc = 1.00
Epoch 268 / 300: Loss = 0.031 Acc = 1.00
Epoch 269 / 300: Loss = 0.031 Acc = 1.00
Epoch 270 / 300: Loss = 0.031 Acc = 1.00
Epoch 271 / 300: Loss = 0.031 Acc = 1.00
Epoch 272 / 300: Loss = 0.030 Acc = 1.00
Epoch 273 / 300: Loss = 0.030 Acc = 1.00
Epoch 274 / 300: Loss = 0.030 Acc = 1.00
Epoch 275 / 300: Loss = 0.030 Acc = 1.00
Epoch 276 / 300: Loss = 0.030 Acc = 1.00
Epoch 277 / 300: Loss = 0.029 Acc = 1.00
Epoch 278 / 300: Loss = 0.029 Acc = 1.00
Epoch 279 / 300: Loss = 0.029 Acc = 1.00
Epoch 280 / 300: Loss = 0.029 Acc = 1.00
Epoch 281 / 300: Loss = 0.028 Acc = 1.00
Epoch 282 / 300: Loss = 0.028 Acc = 1.00
Epoch 283 / 300: Loss = 0.028 Acc = 1.00
Epoch 284 / 300: Loss = 0.028 Acc = 1.00
Epoch 285 / 300: Loss = 0.028 Acc = 1.00
Epoch 286 / 300: Loss = 0.028 Acc = 1.00
Epoch 287 / 300: Loss = 0.027 Acc = 1.00
Epoch 288 / 300: Loss = 0.027 Acc = 1.00
Epoch 289 / 300: Loss = 0.027 Acc = 1.00
Epoch 290 / 300: Loss = 0.027 Acc = 1.00
Epoch 291 / 300: Loss = 0.027 Acc = 1.00
Epoch 292 / 300: Loss = 0.026 Acc = 1.00
Epoch 293 / 300: Loss = 0.026 Acc = 1.00
Epoch 294 / 300: Loss = 0.026 Acc = 1.00
Epoch 295 / 300: Loss = 0.026 Acc = 1.00
Epoch 296 / 300: Loss = 0.026 Acc = 1.00
Epoch 297 / 300: Loss = 0.026 Acc = 1.00
Epoch 298 / 300: Loss = 0.025 Acc = 1.00
Epoch 299 / 300: Loss = 0.025 Acc = 1.00
Epoch 300 / 300: Loss = 0.025 Acc = 1.00
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_result</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">,</span> <span class="n">ix_to_tag</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">prepare_sequence</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
        <span class="n">tag_scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="n">tag_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tag_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">tag_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">ix_to_tag</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">tag_ids</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentence:  </span><span class="si">{</span><span class="n">input_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Labels:    </span><span class="si">{</span><span class="n">labels</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted: </span><span class="si">{</span><span class="n">tag_labels</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_result</span><span class="p">(</span><span class="n">model_classical</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">,</span> <span class="n">ix_to_tag</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Sentence:  [&#39;The&#39;, &#39;dog&#39;, &#39;ate&#39;, &#39;the&#39;, &#39;apple&#39;]
Labels:    [&#39;DET&#39;, &#39;NN&#39;, &#39;V&#39;, &#39;DET&#39;, &#39;NN&#39;]
Predicted: [&#39;DET&#39;, &#39;NN&#39;, &#39;V&#39;, &#39;DET&#39;, &#39;NN&#39;]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_qubits</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">model_quantum</span> <span class="o">=</span> <span class="n">LSTMTagger</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span>
                        <span class="n">hidden_dim</span><span class="p">,</span>
                        <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">),</span>
                        <span class="n">tagset_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tag_to_ix</span><span class="p">),</span>
                        <span class="n">n_qubits</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tagger will use Quantum LSTM
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history_quantum</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model_quantum</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">,</span> <span class="n">tag_to_ix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch 1 / 300: Loss = 0.431 Acc = 0.78
Epoch 2 / 300: Loss = 0.430 Acc = 0.78
Epoch 3 / 300: Loss = 0.428 Acc = 0.78
Epoch 4 / 300: Loss = 0.427 Acc = 0.78
Epoch 5 / 300: Loss = 0.425 Acc = 0.78
Epoch 6 / 300: Loss = 0.423 Acc = 0.78
Epoch 7 / 300: Loss = 0.421 Acc = 0.78
Epoch 8 / 300: Loss = 0.420 Acc = 0.78
Epoch 9 / 300: Loss = 0.418 Acc = 0.78
Epoch 10 / 300: Loss = 0.416 Acc = 0.78
Epoch 11 / 300: Loss = 0.414 Acc = 0.78
Epoch 12 / 300: Loss = 0.412 Acc = 0.78
Epoch 13 / 300: Loss = 0.410 Acc = 0.78
Epoch 14 / 300: Loss = 0.408 Acc = 0.78
Epoch 15 / 300: Loss = 0.406 Acc = 0.78
Epoch 16 / 300: Loss = 0.404 Acc = 0.78
Epoch 17 / 300: Loss = 0.401 Acc = 0.78
Epoch 18 / 300: Loss = 0.399 Acc = 0.78
Epoch 19 / 300: Loss = 0.397 Acc = 0.78
Epoch 20 / 300: Loss = 0.394 Acc = 0.78
Epoch 21 / 300: Loss = 0.392 Acc = 0.78
Epoch 22 / 300: Loss = 0.389 Acc = 0.78
Epoch 23 / 300: Loss = 0.387 Acc = 0.78
Epoch 24 / 300: Loss = 0.384 Acc = 0.78
Epoch 25 / 300: Loss = 0.381 Acc = 0.78
Epoch 26 / 300: Loss = 0.379 Acc = 0.78
Epoch 27 / 300: Loss = 0.376 Acc = 0.78
Epoch 28 / 300: Loss = 0.373 Acc = 0.78
Epoch 29 / 300: Loss = 0.370 Acc = 0.78
Epoch 30 / 300: Loss = 0.368 Acc = 0.78
Epoch 31 / 300: Loss = 0.365 Acc = 0.78
Epoch 32 / 300: Loss = 0.362 Acc = 0.78
Epoch 33 / 300: Loss = 0.359 Acc = 0.78
Epoch 34 / 300: Loss = 0.356 Acc = 0.78
Epoch 35 / 300: Loss = 0.353 Acc = 0.78
Epoch 36 / 300: Loss = 0.350 Acc = 0.78
Epoch 37 / 300: Loss = 0.346 Acc = 0.78
Epoch 38 / 300: Loss = 0.343 Acc = 0.78
Epoch 39 / 300: Loss = 0.340 Acc = 0.78
Epoch 40 / 300: Loss = 0.337 Acc = 0.78
Epoch 41 / 300: Loss = 0.334 Acc = 0.78
Epoch 42 / 300: Loss = 0.330 Acc = 0.78
Epoch 43 / 300: Loss = 0.327 Acc = 0.78
Epoch 44 / 300: Loss = 0.324 Acc = 0.78
Epoch 45 / 300: Loss = 0.320 Acc = 0.78
Epoch 46 / 300: Loss = 0.317 Acc = 0.78
Epoch 47 / 300: Loss = 0.314 Acc = 0.89
Epoch 48 / 300: Loss = 0.311 Acc = 0.89
Epoch 49 / 300: Loss = 0.308 Acc = 0.89
Epoch 50 / 300: Loss = 0.308 Acc = 0.89
Epoch 51 / 300: Loss = 0.316 Acc = 0.89
Epoch 52 / 300: Loss = 0.314 Acc = 0.89
Epoch 53 / 300: Loss = 0.342 Acc = 0.89
Epoch 54 / 300: Loss = 0.299 Acc = 0.89
Epoch 55 / 300: Loss = 0.295 Acc = 1.00
Epoch 56 / 300: Loss = 0.313 Acc = 0.78
Epoch 57 / 300: Loss = 0.288 Acc = 1.00
Epoch 58 / 300: Loss = 0.297 Acc = 0.89
Epoch 59 / 300: Loss = 0.279 Acc = 1.00
Epoch 60 / 300: Loss = 0.287 Acc = 0.89
Epoch 61 / 300: Loss = 0.277 Acc = 1.00
Epoch 62 / 300: Loss = 0.293 Acc = 0.89
Epoch 63 / 300: Loss = 0.269 Acc = 1.00
Epoch 64 / 300: Loss = 0.281 Acc = 0.89
Epoch 65 / 300: Loss = 0.263 Acc = 1.00
Epoch 66 / 300: Loss = 0.271 Acc = 0.89
Epoch 67 / 300: Loss = 0.254 Acc = 1.00
Epoch 68 / 300: Loss = 0.257 Acc = 1.00
Epoch 69 / 300: Loss = 0.251 Acc = 1.00
Epoch 70 / 300: Loss = 0.262 Acc = 0.89
Epoch 71 / 300: Loss = 0.244 Acc = 1.00
Epoch 72 / 300: Loss = 0.252 Acc = 1.00
Epoch 73 / 300: Loss = 0.238 Acc = 1.00
Epoch 74 / 300: Loss = 0.246 Acc = 1.00
Epoch 75 / 300: Loss = 0.231 Acc = 1.00
Epoch 76 / 300: Loss = 0.237 Acc = 1.00
Epoch 77 / 300: Loss = 0.226 Acc = 1.00
Epoch 78 / 300: Loss = 0.231 Acc = 1.00
Epoch 79 / 300: Loss = 0.220 Acc = 1.00
Epoch 80 / 300: Loss = 0.223 Acc = 1.00
Epoch 81 / 300: Loss = 0.214 Acc = 1.00
Epoch 82 / 300: Loss = 0.217 Acc = 1.00
Epoch 83 / 300: Loss = 0.208 Acc = 1.00
Epoch 84 / 300: Loss = 0.210 Acc = 1.00
Epoch 85 / 300: Loss = 0.203 Acc = 1.00
Epoch 86 / 300: Loss = 0.204 Acc = 1.00
Epoch 87 / 300: Loss = 0.197 Acc = 1.00
Epoch 88 / 300: Loss = 0.198 Acc = 1.00
Epoch 89 / 300: Loss = 0.192 Acc = 1.00
Epoch 90 / 300: Loss = 0.192 Acc = 1.00
Epoch 91 / 300: Loss = 0.187 Acc = 1.00
Epoch 92 / 300: Loss = 0.186 Acc = 1.00
Epoch 93 / 300: Loss = 0.182 Acc = 1.00
Epoch 94 / 300: Loss = 0.180 Acc = 1.00
Epoch 95 / 300: Loss = 0.177 Acc = 1.00
Epoch 96 / 300: Loss = 0.175 Acc = 1.00
Epoch 97 / 300: Loss = 0.172 Acc = 1.00
Epoch 98 / 300: Loss = 0.171 Acc = 1.00
Epoch 99 / 300: Loss = 0.168 Acc = 1.00
Epoch 100 / 300: Loss = 0.167 Acc = 1.00
Epoch 101 / 300: Loss = 0.165 Acc = 1.00
Epoch 102 / 300: Loss = 0.163 Acc = 1.00
Epoch 103 / 300: Loss = 0.163 Acc = 1.00
Epoch 104 / 300: Loss = 0.160 Acc = 1.00
Epoch 105 / 300: Loss = 0.159 Acc = 1.00
Epoch 106 / 300: Loss = 0.156 Acc = 1.00
Epoch 107 / 300: Loss = 0.155 Acc = 1.00
Epoch 108 / 300: Loss = 0.153 Acc = 1.00
Epoch 109 / 300: Loss = 0.151 Acc = 1.00
Epoch 110 / 300: Loss = 0.149 Acc = 1.00
Epoch 111 / 300: Loss = 0.147 Acc = 1.00
Epoch 112 / 300: Loss = 0.145 Acc = 1.00
Epoch 113 / 300: Loss = 0.144 Acc = 1.00
Epoch 114 / 300: Loss = 0.142 Acc = 1.00
Epoch 115 / 300: Loss = 0.141 Acc = 1.00
Epoch 116 / 300: Loss = 0.139 Acc = 1.00
Epoch 117 / 300: Loss = 0.138 Acc = 1.00
Epoch 118 / 300: Loss = 0.136 Acc = 1.00
Epoch 119 / 300: Loss = 0.135 Acc = 1.00
Epoch 120 / 300: Loss = 0.134 Acc = 1.00
Epoch 121 / 300: Loss = 0.132 Acc = 1.00
Epoch 122 / 300: Loss = 0.131 Acc = 1.00
Epoch 123 / 300: Loss = 0.130 Acc = 1.00
Epoch 124 / 300: Loss = 0.129 Acc = 1.00
Epoch 125 / 300: Loss = 0.127 Acc = 1.00
Epoch 126 / 300: Loss = 0.126 Acc = 1.00
Epoch 127 / 300: Loss = 0.125 Acc = 1.00
Epoch 128 / 300: Loss = 0.124 Acc = 1.00
Epoch 129 / 300: Loss = 0.123 Acc = 1.00
Epoch 130 / 300: Loss = 0.122 Acc = 1.00
Epoch 131 / 300: Loss = 0.121 Acc = 1.00
Epoch 132 / 300: Loss = 0.119 Acc = 1.00
Epoch 133 / 300: Loss = 0.118 Acc = 1.00
Epoch 134 / 300: Loss = 0.117 Acc = 1.00
Epoch 135 / 300: Loss = 0.116 Acc = 1.00
Epoch 136 / 300: Loss = 0.115 Acc = 1.00
Epoch 137 / 300: Loss = 0.114 Acc = 1.00
Epoch 138 / 300: Loss = 0.113 Acc = 1.00
Epoch 139 / 300: Loss = 0.112 Acc = 1.00
Epoch 140 / 300: Loss = 0.111 Acc = 1.00
Epoch 141 / 300: Loss = 0.111 Acc = 1.00
Epoch 142 / 300: Loss = 0.110 Acc = 1.00
Epoch 143 / 300: Loss = 0.109 Acc = 1.00
Epoch 144 / 300: Loss = 0.108 Acc = 1.00
Epoch 145 / 300: Loss = 0.107 Acc = 1.00
Epoch 146 / 300: Loss = 0.106 Acc = 1.00
Epoch 147 / 300: Loss = 0.105 Acc = 1.00
Epoch 148 / 300: Loss = 0.104 Acc = 1.00
Epoch 149 / 300: Loss = 0.104 Acc = 1.00
Epoch 150 / 300: Loss = 0.103 Acc = 1.00
Epoch 151 / 300: Loss = 0.102 Acc = 1.00
Epoch 152 / 300: Loss = 0.101 Acc = 1.00
Epoch 153 / 300: Loss = 0.100 Acc = 1.00
Epoch 154 / 300: Loss = 0.100 Acc = 1.00
Epoch 155 / 300: Loss = 0.099 Acc = 1.00
Epoch 156 / 300: Loss = 0.098 Acc = 1.00
Epoch 157 / 300: Loss = 0.097 Acc = 1.00
Epoch 158 / 300: Loss = 0.097 Acc = 1.00
Epoch 159 / 300: Loss = 0.096 Acc = 1.00
Epoch 160 / 300: Loss = 0.095 Acc = 1.00
Epoch 161 / 300: Loss = 0.095 Acc = 1.00
Epoch 162 / 300: Loss = 0.094 Acc = 1.00
Epoch 163 / 300: Loss = 0.093 Acc = 1.00
Epoch 164 / 300: Loss = 0.092 Acc = 1.00
Epoch 165 / 300: Loss = 0.092 Acc = 1.00
Epoch 166 / 300: Loss = 0.091 Acc = 1.00
Epoch 167 / 300: Loss = 0.091 Acc = 1.00
Epoch 168 / 300: Loss = 0.090 Acc = 1.00
Epoch 169 / 300: Loss = 0.089 Acc = 1.00
Epoch 170 / 300: Loss = 0.089 Acc = 1.00
Epoch 171 / 300: Loss = 0.088 Acc = 1.00
Epoch 172 / 300: Loss = 0.087 Acc = 1.00
Epoch 173 / 300: Loss = 0.087 Acc = 1.00
Epoch 174 / 300: Loss = 0.086 Acc = 1.00
Epoch 175 / 300: Loss = 0.086 Acc = 1.00
Epoch 176 / 300: Loss = 0.085 Acc = 1.00
Epoch 177 / 300: Loss = 0.085 Acc = 1.00
Epoch 178 / 300: Loss = 0.084 Acc = 1.00
Epoch 179 / 300: Loss = 0.083 Acc = 1.00
Epoch 180 / 300: Loss = 0.083 Acc = 1.00
Epoch 181 / 300: Loss = 0.082 Acc = 1.00
Epoch 182 / 300: Loss = 0.082 Acc = 1.00
Epoch 183 / 300: Loss = 0.081 Acc = 1.00
Epoch 184 / 300: Loss = 0.081 Acc = 1.00
Epoch 185 / 300: Loss = 0.080 Acc = 1.00
Epoch 186 / 300: Loss = 0.080 Acc = 1.00
Epoch 187 / 300: Loss = 0.079 Acc = 1.00
Epoch 188 / 300: Loss = 0.079 Acc = 1.00
Epoch 189 / 300: Loss = 0.078 Acc = 1.00
Epoch 190 / 300: Loss = 0.078 Acc = 1.00
Epoch 191 / 300: Loss = 0.077 Acc = 1.00
Epoch 192 / 300: Loss = 0.077 Acc = 1.00
Epoch 193 / 300: Loss = 0.076 Acc = 1.00
Epoch 194 / 300: Loss = 0.076 Acc = 1.00
Epoch 195 / 300: Loss = 0.075 Acc = 1.00
Epoch 196 / 300: Loss = 0.075 Acc = 1.00
Epoch 197 / 300: Loss = 0.074 Acc = 1.00
Epoch 198 / 300: Loss = 0.074 Acc = 1.00
Epoch 199 / 300: Loss = 0.074 Acc = 1.00
Epoch 200 / 300: Loss = 0.073 Acc = 1.00
Epoch 201 / 300: Loss = 0.073 Acc = 1.00
Epoch 202 / 300: Loss = 0.072 Acc = 1.00
Epoch 203 / 300: Loss = 0.072 Acc = 1.00
Epoch 204 / 300: Loss = 0.071 Acc = 1.00
Epoch 205 / 300: Loss = 0.071 Acc = 1.00
Epoch 206 / 300: Loss = 0.071 Acc = 1.00
Epoch 207 / 300: Loss = 0.070 Acc = 1.00
Epoch 208 / 300: Loss = 0.070 Acc = 1.00
Epoch 209 / 300: Loss = 0.069 Acc = 1.00
Epoch 210 / 300: Loss = 0.069 Acc = 1.00
Epoch 211 / 300: Loss = 0.069 Acc = 1.00
Epoch 212 / 300: Loss = 0.068 Acc = 1.00
Epoch 213 / 300: Loss = 0.068 Acc = 1.00
Epoch 214 / 300: Loss = 0.068 Acc = 1.00
Epoch 215 / 300: Loss = 0.067 Acc = 1.00
Epoch 216 / 300: Loss = 0.067 Acc = 1.00
Epoch 217 / 300: Loss = 0.066 Acc = 1.00
Epoch 218 / 300: Loss = 0.066 Acc = 1.00
Epoch 219 / 300: Loss = 0.066 Acc = 1.00
Epoch 220 / 300: Loss = 0.065 Acc = 1.00
Epoch 221 / 300: Loss = 0.065 Acc = 1.00
Epoch 222 / 300: Loss = 0.065 Acc = 1.00
Epoch 223 / 300: Loss = 0.064 Acc = 1.00
Epoch 224 / 300: Loss = 0.064 Acc = 1.00
Epoch 225 / 300: Loss = 0.064 Acc = 1.00
Epoch 226 / 300: Loss = 0.063 Acc = 1.00
Epoch 227 / 300: Loss = 0.063 Acc = 1.00
Epoch 228 / 300: Loss = 0.063 Acc = 1.00
Epoch 229 / 300: Loss = 0.062 Acc = 1.00
Epoch 230 / 300: Loss = 0.062 Acc = 1.00
Epoch 231 / 300: Loss = 0.062 Acc = 1.00
Epoch 232 / 300: Loss = 0.061 Acc = 1.00
Epoch 233 / 300: Loss = 0.061 Acc = 1.00
Epoch 234 / 300: Loss = 0.061 Acc = 1.00
Epoch 235 / 300: Loss = 0.060 Acc = 1.00
Epoch 236 / 300: Loss = 0.060 Acc = 1.00
Epoch 237 / 300: Loss = 0.060 Acc = 1.00
Epoch 238 / 300: Loss = 0.060 Acc = 1.00
Epoch 239 / 300: Loss = 0.059 Acc = 1.00
Epoch 240 / 300: Loss = 0.059 Acc = 1.00
Epoch 241 / 300: Loss = 0.059 Acc = 1.00
Epoch 242 / 300: Loss = 0.058 Acc = 1.00
Epoch 243 / 300: Loss = 0.058 Acc = 1.00
Epoch 244 / 300: Loss = 0.058 Acc = 1.00
Epoch 245 / 300: Loss = 0.058 Acc = 1.00
Epoch 246 / 300: Loss = 0.057 Acc = 1.00
Epoch 247 / 300: Loss = 0.057 Acc = 1.00
Epoch 248 / 300: Loss = 0.057 Acc = 1.00
Epoch 249 / 300: Loss = 0.056 Acc = 1.00
Epoch 250 / 300: Loss = 0.056 Acc = 1.00
Epoch 251 / 300: Loss = 0.056 Acc = 1.00
Epoch 252 / 300: Loss = 0.056 Acc = 1.00
Epoch 253 / 300: Loss = 0.055 Acc = 1.00
Epoch 254 / 300: Loss = 0.055 Acc = 1.00
Epoch 255 / 300: Loss = 0.055 Acc = 1.00
Epoch 256 / 300: Loss = 0.055 Acc = 1.00
Epoch 257 / 300: Loss = 0.054 Acc = 1.00
Epoch 258 / 300: Loss = 0.054 Acc = 1.00
Epoch 259 / 300: Loss = 0.054 Acc = 1.00
Epoch 260 / 300: Loss = 0.054 Acc = 1.00
Epoch 261 / 300: Loss = 0.053 Acc = 1.00
Epoch 262 / 300: Loss = 0.053 Acc = 1.00
Epoch 263 / 300: Loss = 0.053 Acc = 1.00
Epoch 264 / 300: Loss = 0.053 Acc = 1.00
Epoch 265 / 300: Loss = 0.052 Acc = 1.00
Epoch 266 / 300: Loss = 0.052 Acc = 1.00
Epoch 267 / 300: Loss = 0.052 Acc = 1.00
Epoch 268 / 300: Loss = 0.052 Acc = 1.00
Epoch 269 / 300: Loss = 0.051 Acc = 1.00
Epoch 270 / 300: Loss = 0.051 Acc = 1.00
Epoch 271 / 300: Loss = 0.051 Acc = 1.00
Epoch 272 / 300: Loss = 0.051 Acc = 1.00
Epoch 273 / 300: Loss = 0.051 Acc = 1.00
Epoch 274 / 300: Loss = 0.050 Acc = 1.00
Epoch 275 / 300: Loss = 0.050 Acc = 1.00
Epoch 276 / 300: Loss = 0.050 Acc = 1.00
Epoch 277 / 300: Loss = 0.050 Acc = 1.00
Epoch 278 / 300: Loss = 0.050 Acc = 1.00
Epoch 279 / 300: Loss = 0.049 Acc = 1.00
Epoch 280 / 300: Loss = 0.049 Acc = 1.00
Epoch 281 / 300: Loss = 0.049 Acc = 1.00
Epoch 282 / 300: Loss = 0.049 Acc = 1.00
Epoch 283 / 300: Loss = 0.048 Acc = 1.00
Epoch 284 / 300: Loss = 0.048 Acc = 1.00
Epoch 285 / 300: Loss = 0.048 Acc = 1.00
Epoch 286 / 300: Loss = 0.048 Acc = 1.00
Epoch 287 / 300: Loss = 0.048 Acc = 1.00
Epoch 288 / 300: Loss = 0.047 Acc = 1.00
Epoch 289 / 300: Loss = 0.047 Acc = 1.00
Epoch 290 / 300: Loss = 0.047 Acc = 1.00
Epoch 291 / 300: Loss = 0.047 Acc = 1.00
Epoch 292 / 300: Loss = 0.047 Acc = 1.00
Epoch 293 / 300: Loss = 0.047 Acc = 1.00
Epoch 294 / 300: Loss = 0.046 Acc = 1.00
Epoch 295 / 300: Loss = 0.046 Acc = 1.00
Epoch 296 / 300: Loss = 0.046 Acc = 1.00
Epoch 297 / 300: Loss = 0.046 Acc = 1.00
Epoch 298 / 300: Loss = 0.046 Acc = 1.00
Epoch 299 / 300: Loss = 0.045 Acc = 1.00
Epoch 300 / 300: Loss = 0.045 Acc = 1.00
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_result</span><span class="p">(</span><span class="n">model_quantum</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">,</span> <span class="n">ix_to_tag</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Sentence:  [&#39;The&#39;, &#39;dog&#39;, &#39;ate&#39;, &#39;the&#39;, &#39;apple&#39;]
Labels:    [&#39;DET&#39;, &#39;NN&#39;, &#39;V&#39;, &#39;DET&#39;, &#39;NN&#39;]
Predicted: [&#39;DET&#39;, &#39;NN&#39;, &#39;V&#39;, &#39;DET&#39;, &#39;NN&#39;]
</pre></div></div>
</div>
<section id="Plot-the-training-history">
<h2>Plot the training history<a class="headerlink" href="#Plot-the-training-history" title="Link to this heading">#</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">plot_history</span><span class="p">(</span><span class="n">history_classical</span><span class="p">,</span> <span class="n">history_quantum</span><span class="p">):</span>
    <span class="n">loss_c</span> <span class="o">=</span> <span class="n">history_classical</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
    <span class="n">acc_c</span> <span class="o">=</span> <span class="n">history_classical</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span>
    <span class="n">loss_q</span> <span class="o">=</span> <span class="n">history_quantum</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
    <span class="n">acc_q</span> <span class="o">=</span> <span class="n">history_quantum</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_c</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_q</span><span class="p">)])</span>
    <span class="n">x_epochs</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)]</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_c</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Classical LSTM loss&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_q</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Quantum LSTM loss&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>

    <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">acc_c</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Classical LSTM accuracy&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">acc_q</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Quantum LSTM accuracy&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Part-of-Speech Tagger Training__torch&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="c1">#plt.legend(loc=&quot;upper right&quot;)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.8</span><span class="p">),</span> <span class="n">bbox_transform</span><span class="o">=</span><span class="n">ax1</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;pos_training_torch.pdf&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;pos_training_torch.png&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plot_history</span><span class="p">(</span><span class="n">history_classical</span><span class="p">,</span> <span class="n">history_quantum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/examples_quantum_lstm_qlstm_38_0.png" src="../../_images/examples_quantum_lstm_qlstm_38_0.png" />
</div>
</div>
<p>The loss function decreases as a function of the training epoch, and after 300 epochs both networks are able to tag correctly the first sentence. Due to the complexity of the simulation of the quantum circuit, it took approximatively 15 minutes to finish the training, to be compared to a mere 8 seconds for the classical case.</p>
</section>
</section>
<section id="Refrences">
<h1>Refrences<a class="headerlink" href="#Refrences" title="Link to this heading">#</a></h1>
<p>[1] Riccardo Di Sipio, Jia-Hong Huang, Samuel Yen- Chi Chen, Stefano Mangini, and Marcel Worring. The dawn of quantum natural language processing. In ICASSP 2022-2022 IEEE International Confer- ence on Acoustics, Speech and Signal Processing (ICASSP), pages 8612–8616. IEEE, 2022.</p>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021, Hanrui Wang
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=32e29ea5"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>