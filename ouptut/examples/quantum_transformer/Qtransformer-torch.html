<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Introduction to Transformer - TorchQuantum 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=362ab14a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">TorchQuantum 0.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">TorchQuantum 0.1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_torchquantum.html">torchquantum</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api_functional.html">torchquantum.functional</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of torchquantum.functional</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.apply_unitary_einsum.html">apply_unitary_einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.apply_unitary_bmm.html">apply_unitary_bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.gate_wrapper.html">gate_wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.reset.html">reset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.hadamard.html">hadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.shadamard.html">shadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.paulix.html">paulix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.pauliy.html">pauliy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.pauliz.html">pauliz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.i.html">i</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.s.html">s</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.t.html">t</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.sx.html">sx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cnot.html">cnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cz.html">cz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cy.html">cy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rx.html">rx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ry.html">ry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rz.html">rz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rxx.html">rxx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ryy.html">ryy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rzz.html">rzz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.zz.html">zz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rzx.html">rzx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.zx.html">zx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.swap.html">swap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.sswap.html">sswap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cswap.html">cswap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.toffoli.html">toffoli</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.phaseshift.html">phaseshift</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.p.html">p</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cp.html">cp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.rot.html">rot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.multirz.html">multirz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.crx.html">crx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cry.html">cry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.crz.html">crz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.crot.html">crot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u1.html">u1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u2.html">u2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u3.html">u3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.u.html">u</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu1.html">cu1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu2.html">cu2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu3.html">cu3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cu.html">cu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.qubitunitary.html">qubitunitary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.qubitunitaryfast.html">qubitunitaryfast</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.qubitunitarystrict.html">qubitunitarystrict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.multicnot.html">multicnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.multixcnot.html">multixcnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.x.html">x</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.y.html">y</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.z.html">z</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.zz.html">zz</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.cx.html">cx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ccnot.html">ccnot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ccx.html">ccx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.reset.html">reset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.singleexcitation.html">singleexcitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.ecr.html">ecr</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.functional.echoedcrossresonance.html">echoedcrossresonance</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api_operators.html">torchquantum.operators</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of torchquantum.operators</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.Operator.html">Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.Observable.html">Observable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.Operation.html">Operation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.DiagonalOperation.html">DiagonalOperation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.Hadamard.html">Hadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.SHadamard.html">SHadamard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.PauliX.html">PauliX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.PauliY.html">PauliY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.PauliZ.html">PauliZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.I.html">I</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.S.html">S</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.T.html">T</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.SX.html">SX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CNOT.html">CNOT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CZ.html">CZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CY.html">CY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.RX.html">RX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.RY.html">RY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.RZ.html">RZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.RXX.html">RXX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.RYY.html">RYY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.RZZ.html">RZZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.RZX.html">RZX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.SWAP.html">SWAP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.SSWAP.html">SSWAP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CSWAP.html">CSWAP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.Toffoli.html">Toffoli</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.PhaseShift.html">PhaseShift</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.Rot.html">Rot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.MultiRZ.html">MultiRZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CRX.html">CRX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CRY.html">CRY</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CRZ.html">CRZ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CRot.html">CRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.U1.html">U1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.U2.html">U2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.U3.html">U3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CU1.html">CU1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CU2.html">CU2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.CU3.html">CU3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.QubitUnitary.html">QubitUnitary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.QubitUnitaryFast.html">QubitUnitaryFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.TrainableUnitary.html">TrainableUnitary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.TrainableUnitaryStrict.html">TrainableUnitaryStrict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.MultiCNOT.html">MultiCNOT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.MultiXCNOT.html">MultiXCNOT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.Reset.html">Reset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.SingleExcitation.html">SingleExcitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.EchoedCrossResonance.html">EchoedCrossResonance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.operator.ECR.html">ECR</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api_layers.html">torchquantum.layers</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of torchquantum.layers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.QuantumModuleFromOps.html">QuantumModuleFromOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.TrainableOpAll.html">TrainableOpAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.ClassicalInOpAll.html">ClassicalInOpAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.FixedOpAll.html">FixedOpAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.TwoQAll.html">TwoQAll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.RandomLayer.html">RandomLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.RandomLayerAllTypes.html">RandomLayerAllTypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.Op1QAllLayer.html">Op1QAllLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.RandomOp1All.html">RandomOp1All</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.Op2QAllLayer.html">Op2QAllLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.Op2QButterflyLayer.html">Op2QButterflyLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.Op2QDenseLayer.html">Op2QDenseLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.CXLayer.html">CXLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.CXCXCXLayer.html">CXCXCXLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.SWAPSWAPLayer.html">SWAPSWAPLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.RXYZCXLayer0.html">RXYZCXLayer0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/torchquantum.layer.QFTLayer.html">QFTLayer</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../usage_installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../index.html">TorchQuantum Examples</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of TorchQuantum Examples</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../gradient_pruning/probabilistic_gradient_pruning.html">Probabilistic gradient pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../param_shift_onchip_training/param_shift_onchip_training.html">Apply parameters shift rules to train quantum model using TorchQuantum.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quantum_kernel_method/quantum_kernel_method.html">Quantum Kernel Methods for IRIS dataset classification with TorchQuantum.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quanvolution/quanvolution.html">Quanvolution (Quantum convolution) for MNIST image classification with TorchQuantum.</a></li>
<li class="toctree-l2"><a class="reference internal" href="../superdense_coding/superdense_coding_torchquantum.html">Superdense Coding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../superdense_coding/superdense_coding_torchquantum.html#References:">References:</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <p>Author:Mohammadreza Tavasoli Naeini Advisor:Hanrui Wang</p>
<p>This tutorial shows; how to use the TrochQuantum library in building a quantum transformer. This notebook is adapted, excerpted from <a class="reference external" href="https://github.com/rdisipio/qtransformer">https://github.com/rdisipio/qtransformer</a>, written by Riccardo Di Sipio. In this notebook, we use the TorchQuantum library instead of the Pennylane library to build a quantum machine learning model.</p>
<section id="Introduction-to-Transformer">
<h1>Introduction to Transformer<a class="headerlink" href="#Introduction-to-Transformer" title="Link to this heading">#</a></h1>
<p>Nowadays, Transformer architecture is dominant in the field of sequential data processing, such as natural language processing. Transformers designed to effieciently analyze sequential data on GPUs. Therfore, we see stonishing results in translation, generation from the models build on transformer blocks.[1],[2]</p>
<p>Transformer block does not have memory to sequentially anaylize the data; however, it has position-dependent embedding, which is a kind of word embbeding that embed position of the input in the vector too. Furthermore, trnasformer block use self attation mechanisem to understand which part of the output is related to the input data. The core of transformer block is multi head attetion layer. In multi head atteation layer, three diffrent linear transformer, WQ,WK,and WV, applies to each word
embbeding to transform them to new internal representation called Query (Q), Key (K), and Value (V ). These states are then passed to the function that calculates the attention weights.[1],[2]</p>
<figure><p><img alt="2c6226010f6e4722b61955edbbe9873b" src="examples/quantum_transformer/transformer.png" /></p>
<figcaption align="center"><p>transformer architecture</p>
</figcaption></figure></section>
<section id="Quantum-Transformer">
<h1>Quantum Transformer<a class="headerlink" href="#Quantum-Transformer" title="Link to this heading">#</a></h1>
<p>We convert the classical transformer to the quantum enhanced transformer by replacing the linear transformations WQ, WK, and WV with Varitinal Quantum Circuits. [1]</p>
</section>
<section id="Build-and-train-a-Quantum-Transformer.">
<h1>Build and train a Quantum Transformer.<a class="headerlink" href="#Build-and-train-a-Quantum-Transformer." title="Link to this heading">#</a></h1>
</section>
<section id="Installation">
<h1>Installation<a class="headerlink" href="#Installation" title="Link to this heading">#</a></h1>
<p>Install torchquantum</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mit-han-lab/torchquantum.git
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cloning into &#39;torchquantum&#39;...
remote: Enumerating objects: 11649, done.
remote: Counting objects: 100% (539/539), done.
remote: Compressing objects: 100% (273/273), done.
remote: Total 11649 (delta 315), reused 456 (delta 255), pack-reused 11110
Receiving objects: 100% (11649/11649), 6.43 MiB | 3.37 MiB/s, done.
Resolving deltas: 100% (6472/6472), done.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">cd</span> torchquantum
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
/Users/mohammad/.Trash/torchquantum 10-11-35-421/torchquantum
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--editable<span class="w"> </span>.
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Obtaining file:///Users/mohammad/.Trash/torchquantum%2010-11-35-421/torchquantum
Requirement already satisfied: numpy&gt;=1.19.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (1.20.1)
Requirement already satisfied: torchvision&gt;=0.9.0.dev20210130 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.9.1)
Requirement already satisfied: tqdm&gt;=4.56.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (4.60.0)
Requirement already satisfied: setuptools&gt;=52.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (65.3.0)
Requirement already satisfied: torch&gt;=1.8.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (1.8.1)
Requirement already satisfied: torchpack&gt;=0.3.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.3.1)
Requirement already satisfied: qiskit&gt;=0.32.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.38.0)
Requirement already satisfied: matplotlib&gt;=3.3.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (3.4.1)
Requirement already satisfied: pathos&gt;=0.2.7 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (0.2.9)
Requirement already satisfied: pylatexenc&gt;=2.10 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchquantum==0.1.2) (2.10)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (1.3.1)
Requirement already satisfied: pillow&gt;=6.2.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (8.2.0)
Requirement already satisfied: pyparsing&gt;=2.2.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (2.4.7)
Requirement already satisfied: python-dateutil&gt;=2.7 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (2.8.1)
Requirement already satisfied: cycler&gt;=0.10 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (0.10.0)
Requirement already satisfied: six in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.3.2-&gt;torchquantum==0.1.2) (1.15.0)
Requirement already satisfied: ppft&gt;=1.7.6.5 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (1.7.6.5)
Requirement already satisfied: dill&gt;=0.3.5.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (0.3.5.1)
Requirement already satisfied: multiprocess&gt;=0.70.13 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (0.70.13)
Requirement already satisfied: pox&gt;=0.3.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pathos&gt;=0.2.7-&gt;torchquantum==0.1.2) (0.3.1)
Requirement already satisfied: qiskit-aer==0.11.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.11.0)
Requirement already satisfied: qiskit-terra==0.21.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.21.2)
Requirement already satisfied: qiskit-ibmq-provider==0.19.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.19.2)
Requirement already satisfied: scipy&gt;=1.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-aer==0.11.0-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.6.1)
Requirement already satisfied: urllib3&gt;=1.21.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.26.4)
Requirement already satisfied: requests&gt;=2.19 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2.25.1)
Requirement already satisfied: websockets&gt;=10.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (10.3)
Requirement already satisfied: websocket-client&gt;=1.0.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.4.1)
Requirement already satisfied: requests-ntlm&gt;=1.1.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.1.0)
Requirement already satisfied: psutil&gt;=5 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (5.8.0)
Requirement already satisfied: sympy&gt;=1.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.7.1)
Requirement already satisfied: retworkx&gt;=0.11.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.11.0)
Requirement already satisfied: tweedledum&lt;2.0,&gt;=1.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.1.1)
Requirement already satisfied: stevedore&gt;=3.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (4.0.0)
Requirement already satisfied: symengine&gt;=0.9 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (0.9.2)
Requirement already satisfied: ply&gt;=3.10 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (3.11)
Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests&gt;=2.19-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2020.12.5)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests&gt;=2.19-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (4.0.0)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests&gt;=2.19-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2.10)
Requirement already satisfied: ntlm-auth&gt;=1.0.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.5.0)
Requirement already satisfied: cryptography&gt;=1.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (3.4.7)
Requirement already satisfied: cffi&gt;=1.12 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from cryptography&gt;=1.3-&gt;requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.14.5)
Requirement already satisfied: pycparser in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=1.3-&gt;requests-ntlm&gt;=1.1.0-&gt;qiskit-ibmq-provider==0.19.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (2.20)
Requirement already satisfied: pbr!=2.1.0,&gt;=2.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from stevedore&gt;=3.0.0-&gt;qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (5.10.0)
Requirement already satisfied: mpmath&gt;=0.19 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from sympy&gt;=1.3-&gt;qiskit-terra==0.21.2-&gt;qiskit&gt;=0.32.0-&gt;torchquantum==0.1.2) (1.2.1)
Requirement already satisfied: typing-extensions in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torch&gt;=1.8.0-&gt;torchquantum==0.1.2) (3.7.4.3)
Requirement already satisfied: multimethod in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.8)
Requirement already satisfied: tensorboard in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (2.9.1)
Requirement already satisfied: tensorpack in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.11)
Requirement already satisfied: h5py in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.1.0)
Requirement already satisfied: pyyaml in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (5.3.1)
Requirement already satisfied: toml in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.10.2)
Requirement already satisfied: loguru in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.6.0)
Requirement already satisfied: wheel&gt;=0.26 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.36.2)
Requirement already satisfied: grpcio&gt;=1.24.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.37.0)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.28.0)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.8.0)
Requirement already satisfied: protobuf&lt;3.20,&gt;=3.9.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.15.7)
Requirement already satisfied: absl-py&gt;=0.4 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.2.0)
Requirement already satisfied: markdown&gt;=2.6.8 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.3.4)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.6.1)
Requirement already satisfied: werkzeug&gt;=1.0.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.0.1)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.4.4)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (4.7.2)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.2.8)
Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (4.2.1)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.3.0)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.4.8)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (3.1.0)
Requirement already satisfied: pyzmq&gt;=16 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (22.3.0)
Requirement already satisfied: tabulate&gt;=0.7.7 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.8.10)
Requirement already satisfied: msgpack&gt;=0.5.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.0.4)
Requirement already satisfied: termcolor&gt;=1.1 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (1.1.0)
Requirement already satisfied: msgpack-numpy&gt;=0.4.4.2 in /Users/mohammad/miniforge3/lib/python3.8/site-packages (from tensorpack-&gt;torchpack&gt;=0.3.0-&gt;torchquantum==0.1.2) (0.4.8)
Installing collected packages: torchquantum
  Attempting uninstall: torchquantum
    Found existing installation: torchquantum 0.1.2
    Uninstalling torchquantum-0.1.2:
      Successfully uninstalled torchquantum-0.1.2
  Running setup.py develop for torchquantum
Successfully installed torchquantum
</pre></div></div>
</div>
</section>
<section id="Setup">
<h1>Setup<a class="headerlink" href="#Setup" title="Link to this heading">#</a></h1>
<p>Our code requires torchquantum lib, and pytorch. We need torch, function from torch.nn.functional, optimizers(optim), torchquantum module, and torchtext.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="kn">import</span> <span class="nn">torchquantum</span> <span class="k">as</span> <span class="nn">tq</span>
<span class="kn">import</span> <span class="nn">torchquantum.functional</span> <span class="k">as</span> <span class="nn">tqf</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">tqdm</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchtext.legacy</span>
<span class="kn">from</span> <span class="nn">torchtext.legacy</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">vocab</span>
</pre></div>
</div>
</div>
</section>
<section id="Build-MultiHead-Attention">
<h1>Build MultiHead Attention<a class="headerlink" href="#Build-MultiHead-Attention" title="Link to this heading">#</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttentionBase</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttentionBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Embedding dimension (</span><span class="si">{</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">) should be divisible by number of heads (</span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">)&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>  <span class="c1"># projection dimensions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_linear</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_linear</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_linear</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">combine_heads</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_weights</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">separate_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        split into N heads</span>
<span class="sd">        from (batch_size, seq_len, embed_dim)</span>
<span class="sd">        to   (batch_size, seq_len, num_heads, embed_dim)</span>
<span class="sd">        then transpose (1,2) to (batch_size, num_heads, seq_len, embed_dim)</span>
<span class="sd">        to make mat mult straightforward for each head</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k))V</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="c1"># see also: https://tensorchiefs.github.io/dlday2018/tutorial/einsum.html</span>
        <span class="c1">#scores = torch.einsum(&#39;bijh, bkjh -&gt; bikh&#39;, query, key) / math.sqrt(self.d_k)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attn</span><span class="p">,</span> <span class="n">scores</span>

    <span class="k">def</span> <span class="nf">downstream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">separate_heads</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">separate_heads</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">separate_heads</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">concat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">concat</span>
        <span class="c1"># output = self.combine_heads(concat)</span>
        <span class="c1"># return output</span>

   <span class="c1"># def forward(self, x, mask=None):</span>
    <span class="c1">#    raise NotImplementedError(&quot;Base class does not execute forward function.&quot;)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttentionClassical</span><span class="p">(</span><span class="n">MultiHeadAttentionBase</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttentionClassical</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">k_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">combine_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Input embedding (</span><span class="si">{</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">) does not match layer embedding size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">)&quot;</span>

        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downstream</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">combine_heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
<p>Typically, a Quantum Neural Network module consists of three parts: encoder, ansatz, and measurement. We can create an encoder by passing a list of gates to tq.GeneralEncoder. Each entry in the list contains input_idx, func, and wires. Here, each qubit has a rotation-X gate. 8 RX gates in total. They can encode the 8 input data to the quantum state. Then we choose ansatz such that they are entangled between each other, rotated by an arbitrary angle. Finally, we perform Pauli-Z measurements on
each qubit by creating a tq.MeasureAll module and passing tq.PauliZ to it. The measure function will return four expectation values from four qubits.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttentionQuantum</span><span class="p">(</span><span class="n">MultiHeadAttentionBase</span><span class="p">):</span>
    <span class="k">class</span> <span class="nc">QLayer</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">QuantumModule</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span> <span class="o">=</span> <span class="mi">8</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">GeneralEncoder</span><span class="p">(</span>
        <span class="p">[</span>   <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]},</span>
             <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">]},</span>
          <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">7</span><span class="p">]},</span>
        <span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx4</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx5</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx6</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx7</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">measure</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">MeasureAll</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">q_device</span><span class="p">:</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_device</span> <span class="o">=</span> <span class="n">q_device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx6</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx7</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_qubits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                 <span class="n">n_qlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">q_device</span><span class="o">=</span><span class="s2">&quot;default.qubit&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttentionQuantum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>

        <span class="c1"># todo: add intermediate layer to &quot;dress&quot; quantum circuit</span>
        <span class="k">assert</span> <span class="n">n_qubits</span> <span class="o">==</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="s2">&quot;Number of qubits (</span><span class="si">{n_qubits}</span><span class="s2">) does not match embedding dim (</span><span class="si">{embed_dim}</span><span class="s2">)&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qubits</span> <span class="o">=</span> <span class="n">n_qubits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qlayers</span> <span class="o">=</span> <span class="n">n_qlayers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dev</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_qubits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QLayer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">measure</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">MeasureAll</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Input embedding (</span><span class="si">{</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">) does not match layer embedding size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">)&quot;</span>

        <span class="n">K</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">q_layer</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)]</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">q_layer</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)]</span>
        <span class="n">V</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">q_layer</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)]</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">Q</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downstream</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">q_layer</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:],</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">output</span><span class="p">))</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</section>
<section id="Build-FeedForwrad-Layer">
<h1>Build FeedForwrad Layer<a class="headerlink" href="#Build-FeedForwrad-Layer" title="Link to this heading">#</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FeedForwardBase</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForwardBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ffn_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1">#def forward(self, x):</span>
     <span class="c1">#   raise NotImplementedError(&quot;Base class does not implement forward function&quot;)</span>


<span class="k">class</span> <span class="nc">FeedForwardClassical</span><span class="p">(</span><span class="n">FeedForwardBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForwardClassical</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<p>Here, similar to what we have done for the multi-head attention layer, we build the FeedForward quantum layer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FeedForwardQuantum</span><span class="p">(</span><span class="n">FeedForwardBase</span><span class="p">):</span>
    <span class="k">class</span> <span class="nc">QL</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">QuantumModule</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span> <span class="o">=</span> <span class="mi">8</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">GeneralEncoder</span><span class="p">(</span>
        <span class="p">[</span>   <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">]},</span>
            <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]},</span>
             <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">]},</span>
          <span class="p">{</span><span class="s1">&#39;input_idx&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="s1">&#39;func&#39;</span><span class="p">:</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="s1">&#39;wires&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">7</span><span class="p">]},</span>
        <span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx4</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx5</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx6</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx7</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">RX</span><span class="p">(</span><span class="n">has_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">measure</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">MeasureAll</span><span class="p">(</span><span class="n">tq</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">q_device</span><span class="p">:</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_device</span> <span class="o">=</span> <span class="n">q_device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx0</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx6</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rx7</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">n_wires</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tqf</span><span class="o">.</span><span class="n">cnot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_device</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_qubits</span><span class="p">,</span> <span class="n">n_qlayers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">q_device</span><span class="o">=</span><span class="s2">&quot;default.qubit&quot;</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForwardQuantum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">n_qubits</span> <span class="o">==</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="s2">&quot;Number of qubits (</span><span class="si">{n_qubits}</span><span class="s2">) does not match embedding dim (</span><span class="si">{embed_dim}</span><span class="s2">)&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qubits</span> <span class="o">=</span> <span class="n">n_qubits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qlayers</span> <span class="o">=</span> <span class="n">n_qlayers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dev</span> <span class="o">=</span> <span class="n">tq</span><span class="o">.</span><span class="n">QuantumDevice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_qubits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QL</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">o</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">q_l</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">o</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</section>
<section id="Build-the-whole-hybrid-transformer-model.">
<h1>Build the whole hybrid transformer model.<a class="headerlink" href="#Build-the-whole-hybrid-transformer-model." title="Link to this heading">#</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformerBlockBase</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">ff_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">n_qubits_transformer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_qubits_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_qlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerBlockBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">attn_output</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">ff_output</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
<span class="k">class</span> <span class="nc">TransformerBlockClassical</span><span class="p">(</span><span class="n">TransformerBlockBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">ff_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerBlockClassical</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadAttentionClassical</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForwardClassical</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">TransformerBlockQuantum</span><span class="p">(</span><span class="n">TransformerBlockBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">ffn_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">n_qubits_transformer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_qubits_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_qlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">q_device</span><span class="o">=</span><span class="s1">&#39;default.qubit&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerBlockQuantum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_qubits_transformer</span> <span class="o">=</span> <span class="n">n_qubits_transformer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qubits_ffn</span> <span class="o">=</span> <span class="n">n_qubits_ffn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_qlayers</span> <span class="o">=</span> <span class="n">n_qlayers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadAttentionQuantum</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span>
                                              <span class="n">num_heads</span><span class="p">,</span>
                                              <span class="n">n_qubits</span><span class="o">=</span><span class="n">n_qubits_transformer</span><span class="p">,</span>
                                              <span class="n">n_qlayers</span><span class="o">=</span><span class="n">n_qlayers</span><span class="p">,</span>
                                              <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                                              <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
                                              <span class="n">q_device</span><span class="o">=</span><span class="n">q_device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_qubits_ffn</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForwardQuantum</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_qubits_ffn</span><span class="p">,</span> <span class="n">n_qlayers</span><span class="p">,</span> <span class="n">q_device</span><span class="o">=</span><span class="n">q_device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FeedForwardClassical</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PositionalEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

        <span class="c1"># create constant &#39;pe&#39; matrix with values dependant on pos and i</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">embed_dim</span><span class="p">)))</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">embed_dim</span><span class="p">)))</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># make embeddings relatively larger</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="c1">#add constant to embedding</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,:</span><span class="n">seq_len</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># .cuda()</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">TextClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">ffn_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                 <span class="n">n_qubits_transformer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_qubits_ffn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_qlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">q_device</span><span class="o">=</span><span class="s2">&quot;device.qubit&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TextClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_blocks</span> <span class="o">=</span> <span class="n">num_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span> <span class="o">=</span> <span class="n">PositionalEncoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;++ There will be </span><span class="si">{</span><span class="n">num_blocks</span><span class="si">}</span><span class="s2"> transformer blocks&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_qubits_transformer</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;++ Transformer will use </span><span class="si">{</span><span class="n">n_qubits_transformer</span><span class="si">}</span><span class="s2"> qubits and </span><span class="si">{</span><span class="n">n_qlayers</span><span class="si">}</span><span class="s2"> q layers&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">n_qubits_ffn</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The feed-forward head will use </span><span class="si">{</span><span class="n">n_qubits_ffn</span><span class="si">}</span><span class="s2"> qubits&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The feed-forward head will be classical&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using quantum device </span><span class="si">{</span><span class="n">q_device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="n">transformer_blocks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">TransformerBlockQuantum</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">,</span>
                                        <span class="n">n_qubits_transformer</span><span class="o">=</span><span class="n">n_qubits_transformer</span><span class="p">,</span>
                                        <span class="n">n_qubits_ffn</span><span class="o">=</span><span class="n">n_qubits_ffn</span><span class="p">,</span>
                                        <span class="n">n_qlayers</span><span class="o">=</span><span class="n">n_qlayers</span><span class="p">,</span>
                                        <span class="n">q_device</span><span class="o">=</span><span class="n">q_device</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">transformer_blocks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">TransformerBlockClassical</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transformers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">transformer_blocks</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_logits</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_logits</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># batch_size, seq_len, embed_dim = x.size()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># global average pooling, works in 1D</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># x = self.class_logits(x)</span>
        <span class="c1"># return F.log_softmax(x, dim=1)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_logits</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<br/><br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">binary_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">#round predictions to the closest integer</span>
    <span class="n">rounded_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">preds</span><span class="p">))</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">rounded_preds</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1">#convert into float for division</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">acc</span>
<br/><br/><br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">iterator</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_detect_anomaly</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="p">:</span><span class="n">MAX_SEQ_LEN</span><span class="p">]</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="c1">#label = label.unsqueeze(1)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="c1">#loss = F.nll_loss(predictions, label)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">binary_accuracy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">epoch_acc</span> <span class="o">+=</span> <span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">),</span> <span class="n">epoch_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
<br/><br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">iterator</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>

    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="p">:</span><span class="n">MAX_SEQ_LEN</span><span class="p">]</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">label</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="c1">#label = label.unsqueeze(1)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="c1">#loss = F.nll_loss(predictions, label)</span>
            <span class="n">acc</span> <span class="o">=</span> <span class="n">binary_accuracy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">epoch_acc</span> <span class="o">+=</span> <span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">),</span> <span class="n">epoch_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
<br/><br/><br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">epoch_time</span><span class="p">(</span><span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">):</span>
    <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">elapsed_mins</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">elapsed_time</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">elapsed_secs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">elapsed_time</span> <span class="o">-</span> <span class="p">(</span><span class="n">elapsed_mins</span> <span class="o">*</span> <span class="mi">60</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">elapsed_mins</span><span class="p">,</span> <span class="n">elapsed_secs</span>
<br/><br/><br/></pre></div>
</div>
</div>
</section>
<section id="Training-Classical-transformer">
<h1>Training Classical transformer<a class="headerlink" href="#Training-Classical-transformer" title="Link to this heading">#</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>

    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-D&#39;</span><span class="p">,</span> <span class="s1">&#39;--q_device&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;local&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-B&#39;</span><span class="p">,</span> <span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-E&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_epochs&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-C&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_classes&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-l&#39;</span><span class="p">,</span> <span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-v&#39;</span><span class="p">,</span> <span class="s1">&#39;--vocab_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-e&#39;</span><span class="p">,</span> <span class="s1">&#39;--embed_dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-s&#39;</span><span class="p">,</span> <span class="s1">&#39;--max_seq_len&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f&#39;</span><span class="p">,</span> <span class="s1">&#39;--ffn_dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-t&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_transformer_blocks&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-H&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_heads&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-q&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_qubits_transformer&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-Q&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_qubits_ffn&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-L&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_qlayers&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-d&#39;</span><span class="p">,</span> <span class="s1">&#39;--dropout_rate&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[])</span>

    <span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span>

    <span class="n">TEXT</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">include_lengths</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1">#LABEL = data.Field(sequential=False)</span>
    <span class="n">LABEL</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">LabelField</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">IMDB</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Training examples: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Testing examples:  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># exclude &lt;UNK&gt; and &lt;PAD&gt;</span>
    <span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

    <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">((</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">TextClassifier</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
                           <span class="n">num_heads</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
                           <span class="n">num_blocks</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_transformer_blocks</span><span class="p">,</span>
                           <span class="n">num_classes</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_classes</span><span class="p">,</span>
                           <span class="n">vocab_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
                           <span class="n">ffn_dim</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ffn_dim</span><span class="p">,</span>
                           <span class="n">n_qubits_transformer</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_qubits_transformer</span><span class="p">,</span>
                           <span class="n">n_qubits_ffn</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_qubits_ffn</span><span class="p">,</span>
                           <span class="n">n_qlayers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_qlayers</span><span class="p">,</span>
                           <span class="n">dropout</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                           <span class="n">q_device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">q_device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The model has </span><span class="si">{</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1"> trainable parameters&#39;</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>  <span class="c1"># logits -&gt; sigmoid -&gt; loss</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># logits -&gt; log_softmax -&gt; NLLloss</span>

    <span class="c1"># training loop</span>
    <span class="n">best_valid_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iepoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">iepoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">n_epochs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
        <span class="n">valid_loss</span><span class="p">,</span> <span class="n">valid_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>

        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">epoch_mins</span><span class="p">,</span> <span class="n">epoch_secs</span> <span class="o">=</span> <span class="n">epoch_time</span><span class="p">(</span><span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">valid_loss</span> <span class="o">&lt;</span> <span class="n">best_valid_loss</span><span class="p">:</span>
            <span class="n">best_valid_loss</span> <span class="o">=</span> <span class="n">valid_loss</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;model.pt&#39;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">iepoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s1">02</span><span class="si">}</span><span class="s1"> | Epoch Time: </span><span class="si">{</span><span class="n">epoch_mins</span><span class="si">}</span><span class="s1">m </span><span class="si">{</span><span class="n">epoch_secs</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> | Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1"> Val. Loss: </span><span class="si">{</span><span class="n">valid_loss</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> |  Val. Acc: </span><span class="si">{</span><span class="n">valid_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
<br/><br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training examples: 25000
Testing examples:  25000
++ There will be 1 transformer blocks
The model has 160,441 trainable parameters
Epoch 1/5
Epoch: 01 | Epoch Time: 0m 28s
        Train Loss: -5.346 | Train Acc: 49.22%
         Val. Loss: -13.767 |  Val. Acc: 50.04%
Epoch 2/5
Epoch: 02 | Epoch Time: 0m 31s
        Train Loss: -28.989 | Train Acc: 50.00%
         Val. Loss: -47.243 |  Val. Acc: 50.04%
Epoch 3/5
Epoch: 03 | Epoch Time: 0m 28s
        Train Loss: -70.884 | Train Acc: 50.01%
         Val. Loss: -97.031 |  Val. Acc: 50.04%
Epoch 4/5
Epoch: 04 | Epoch Time: 0m 28s
        Train Loss: -127.913 | Train Acc: 50.01%
         Val. Loss: -160.671 |  Val. Acc: 50.04%
Epoch 5/5
Epoch: 05 | Epoch Time: 0m 28s
        Train Loss: -197.544 | Train Acc: 50.01%
         Val. Loss: -236.139 |  Val. Acc: 50.04%
</pre></div></div>
</div>
</section>
<section id="Training-Quantum-transformer">
<h1>Training Quantum transformer<a class="headerlink" href="#Training-Quantum-transformer" title="Link to this heading">#</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>

    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-D&#39;</span><span class="p">,</span> <span class="s1">&#39;--q_device&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;default.qubit&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-B&#39;</span><span class="p">,</span> <span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-E&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_epochs&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-C&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_classes&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-l&#39;</span><span class="p">,</span> <span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-v&#39;</span><span class="p">,</span> <span class="s1">&#39;--vocab_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-e&#39;</span><span class="p">,</span> <span class="s1">&#39;--embed_dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-s&#39;</span><span class="p">,</span> <span class="s1">&#39;--max_seq_len&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f&#39;</span><span class="p">,</span> <span class="s1">&#39;--ffn_dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-t&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_transformer_blocks&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-H&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_heads&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-q&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_qubits_transformer&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-Q&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_qubits_ffn&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-L&#39;</span><span class="p">,</span> <span class="s1">&#39;--n_qlayers&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-d&#39;</span><span class="p">,</span> <span class="s1">&#39;--dropout_rate&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[])</span>

    <span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span>

    <span class="n">TEXT</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">include_lengths</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1">#LABEL = data.Field(sequential=False)</span>
    <span class="n">LABEL</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">LabelField</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">IMDB</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Training examples: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Testing examples:  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># exclude &lt;UNK&gt; and &lt;PAD&gt;</span>
    <span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

    <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">((</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">TextClassifier</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
                           <span class="n">num_heads</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
                           <span class="n">num_blocks</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_transformer_blocks</span><span class="p">,</span>
                           <span class="n">num_classes</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_classes</span><span class="p">,</span>
                           <span class="n">vocab_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
                           <span class="n">ffn_dim</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ffn_dim</span><span class="p">,</span>
                           <span class="n">n_qubits_transformer</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_qubits_transformer</span><span class="p">,</span>
                           <span class="n">n_qubits_ffn</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_qubits_ffn</span><span class="p">,</span>
                           <span class="n">n_qlayers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_qlayers</span><span class="p">,</span>
                           <span class="n">dropout</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                           <span class="n">q_device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">q_device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The model has </span><span class="si">{</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1"> trainable parameters&#39;</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>  <span class="c1"># logits -&gt; sigmoid -&gt; loss</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># logits -&gt; log_softmax -&gt; NLLloss</span>

    <span class="c1"># training loop</span>
    <span class="n">best_valid_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iepoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">iepoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">n_epochs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
        <span class="n">valid_loss</span><span class="p">,</span> <span class="n">valid_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>

        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">epoch_mins</span><span class="p">,</span> <span class="n">epoch_secs</span> <span class="o">=</span> <span class="n">epoch_time</span><span class="p">(</span><span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">valid_loss</span> <span class="o">&lt;</span> <span class="n">best_valid_loss</span><span class="p">:</span>
            <span class="n">best_valid_loss</span> <span class="o">=</span> <span class="n">valid_loss</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;model.pt&#39;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">iepoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s1">02</span><span class="si">}</span><span class="s1"> | Epoch Time: </span><span class="si">{</span><span class="n">epoch_mins</span><span class="si">}</span><span class="s1">m </span><span class="si">{</span><span class="n">epoch_secs</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> | Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1"> Val. Loss: </span><span class="si">{</span><span class="n">valid_loss</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> |  Val. Acc: </span><span class="si">{</span><span class="n">valid_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
<br/><br/><br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training examples: 25000
Testing examples:  25000
++ There will be 1 transformer blocks
++ Transformer will use 8 qubits and 1 q layers
The feed-forward head will use 8 qubits
Using quantum device default.qubit
The model has 400,201 trainable parameters
Epoch 1/1
</pre></div></div>
</div>
</section>
<section id="Refrences">
<h1>Refrences<a class="headerlink" href="#Refrences" title="Link to this heading">#</a></h1>
<p>[1] Riccardo Di Sipio, Jia-Hong Huang, Samuel Yen- Chi Chen, Stefano Mangini, and Marcel Worring. The dawn of quantum natural language processing. In ICASSP 2022-2022 IEEE International Confer- ence on Acoustics, Speech and Signal Processing (ICASSP), pages 86128616. IEEE, 2022.</p>
<p>[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 59986008, 2017.</p>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021, Hanrui Wang
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=32e29ea5"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>